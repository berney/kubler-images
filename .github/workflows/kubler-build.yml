name: Kubler Build
on:
  #push:
  #  branches:
  #    - main
  workflow_dispatch:
    inputs:
      portage_date:
        description: The date (tag) of the latest portage image (e.g. `20241231`)
        required: true
        # Could be `latest` or a date like `20241231`
        type: string
        default: latest
      portage_image:
        description: Portage Image (e.g. `ghcr.io/berney/kubler-images/portage:20241231`)
        required: true
        type: string
        default: ghcr.io/berney/kubler-images/portage:latest
  workflow_call:
    inputs:
      portage_date:
        description: The date (tag) of the latest portage image (e.g. `20241231`)
        required: true
        # Could be `latest` or a date like `20241231`
        type: string
      portage_image:
        description: Portage Image (e.g. `ghcr.io/berney/kubler-images/portage:20241231`)
        required: true
        type: string
    secrets:
      LIGOLO_PROXY:
        description: Ligolo-ng proxy (server) address
        required: false
      AZURE_CLIENT_ID:
        description: Azure Client ID of a service principal or a user-assigned managed identity
        required: true
      AZURE_TENANT_ID:
        description: Azure login Tenant ID
        required: true
      AZURE_SUBSCRIPTION_ID:
        description: Azure login Subscription ID
        required: true
      AZURE_STORAGE_ACCOUNT:
        description: Azure Storage Account for Blob Storage, for shared caching of Gentoo binpkgs
        required: true
      AZURE_STORAGE_CONTAINER:
        description: Azure Storage Container for Blob Storage, for shared caching of Gentoo binpkgs
        required: true

permissions:
  id-token: write
  contents: read
  packages: write

# Jobs run in parallel
# Jobs are independent with separate file systems, IP addresses, etc.
jobs:
  kubler:
    runs-on: ubuntu-latest
    env:
      KUBLER_IMAGE: ghcr.io/${{ github.repository }}
      PORTAGE_DATE: ${{ inputs.portage_date || 'latest' }}
      PORTAGE_IMAGE: ${{ inputs.portage_image || format('ghcr.io/{0}/portage:{1}', github.repository, inputs.portage_date || 'latest') }}
      STAGE3_IMAGE: ghcr.io/${{ github.repository }}/stage3-amd64-hardened-nomultilib-openrc:${{ inputs.portage_date || 'latest' }}
      STAGE3_MUSL_IMAGE: ghcr.io/${{ github.repository }}/stage3-amd64-musl-hardened:${{ inputs.portage_date || 'latest' }}
      BOB_CORE_IMAGE: ghcr.io/${{ github.repository }}/bob-core:${{ inputs.portage_date || 'latest' }}
      BOB_MUSL_CORE_IMAGE: ghcr.io/${{ github.repository }}/bob-musl-core:${{ inputs.portage_date || 'latest' }}
      BOB_IMAGE: ghcr.io/${{ github.repository }}/bob:${{ inputs.portage_date || 'latest' }}
      BOB_MUSL_IMAGE: ghcr.io/${{ github.repository }}/bob-musl:${{ inputs.portage_date || 'latest' }}
    continue-on-error: ${{ matrix.experimental || false }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: busybox
            image: kubler-images/busybox
            bob-musl: true
            goss:
              dir: images/busybox
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: kubler/busybox
            image: kubler/busybox
            bob-musl: true
            goss:
              dir: images/busybox
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: kubler/glibc
            image: kubler/glibc
            bob: true
            bob-musl: true
            goss:
              dir: images/glibc
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet
            image: kubler-images/figlet
            bob: true
            bob-musl: true
            goss:
              dir: images/figlet
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-user
            image: kubler-images/figlet-user
            bob: true
            bob-musl: true
            goss:
              dir: images/figlet-user
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-musl
            image: kubler-images/figlet-musl
            bob-musl: true
            goss:
              dir: images/figlet-musl
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-musl-static
            image: kubler-images/figlet-musl-static
            bob-musl: true
            goss:
              dir: images/figlet-musl-static
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: goss
            image: kubler-images/goss
            bob-musl: true
            goss:
              dir: images/goss
              container_mode: entrypoint
              # Because the image is already using `/goss` (and it's a file), we need `dgoss` to use a different directory
              # I've added the `CONTAINER_GOSS_PATH` env var to allow overriding the default
              container_goss_path: /goss2
              docker_args: -w /goss2 --entrypoint /goss2/goss
              goss_args: validate --color
          - name: fd
            image: kubler-images/fd
            bob-musl: true
            goss:
              dir: images/fd
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: s6
            image: kubler-images/s6
            bob-musl: true
            push: true
            goss:
              dir: images/s6
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: s6-busybox
            image: kubler-images/s6-busybox
            bob-musl: true
            push: true
            goss:
              dir: images/s6-busybox
              container_mode: inject
              # ash needs a terminal to start
              docker_args: -t
              goss_args:
          - name: coturn
            image: kubler-images/coturn
            bob-musl: true
            #push: true
            #goss:
            #  dir: images/coturn
            #  container_mode: inject
            #  # ash needs a terminal to start
            #  docker_args: -t
            #  goss_args:
          - name: s6-coturn-busybox
            image: kubler-images/s6-coturn-busybox
            bob-musl: true
            #push: true
            #goss:
            #  dir: images/coturn
            #  container_mode: inject
            #  # ash needs a terminal to start
            #  docker_args: -t
            #  goss_args:
          - name: nmap
            image: kubler-images/nmap
            bob: true
            bob-musl: true
            #push: true
            # extra test: docker run --rm kubler-images/nmap -A --open scanme.nmap.org
            goss:
              dir: images/nmap
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: -l debug validate --color
          - name: nmap-musl-static
            experimental: true
            image: kubler-images/nmap-musl-static
            bob-musl: true
            push: true
            # extra test: docker run --rm kubler-images/nmap-musl-static -A --open scanme.nmap.org
            goss:
              dir: images/nmap-musl-static
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: -l debug validate --color
          - name: tmux
            image: kubler-images/tmux
            bob-musl: true
            #goss:
            #  dir: images/tmux
            #  container_mode: entrypoint
            #  docker_args: -w /goss --entrypoint /goss/goss
            #  goss_args: validate --color

    steps:
      - name: Inspect
        run: |
          set -eux
          id
          uname
          pwd
          echo "${{ github.workspace }}"
          echo $HOME
          ls -la
          ls -la /
          cd
          pwd
          mount

      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Update Docker Engine
        # Disable
        if: ${{ false }}
        run: |
          docker version
          docker info
          # Remove old packages
          for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg || true; done
          sudo apt-get update
          sudo apt-get install ca-certificates curl gnupg
          ls -ld /etc/apt/keyrings || true
          ls -l /etc/apt/keyrings/docker.gpg || true
          sudo install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          sudo chmod a+r /etc/apt/keyrings/docker.gpg
          ls -ld /etc/apt/sources.list.d || true
          ls -l /etc/apt/sources.list.d/ || true
          ls -l /etc/apt/sources.list.d/docker.list || true
          echo \
            "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
            "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
            sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
          cat /etc/apt/sources.list.d/docker.list
          sudo apt-get update
          sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
          systemctl status docker
          sudo systemctl start docker
          docker version
          docker info
          docker run --rm hello-world

      # Cache Versions are based off key and path, so differnt path's can use same key
      #
      # Caches are immutable, so need unique key to create a new cache
      # `restore-keys` provides a list to restore a cache when key doesn't match
      # If there's no exact match, the most recent cache that partially matches will be used
      #
      - name: Cache Kubler Downloads
        uses: actions/cache@v4
        with:
          path: ~/.kubler/downloads/
          key: kubler-${{ matrix.name }}-${{ github.sha }}
          restore-keys: |
            kubler-${{ matrix.name }}-
            kubler-

      - name: Cache Kubler Gentoo Distfiles
        uses: actions/cache@v4
        with:
          path: ~/.kubler/distfiles/
          key: kubler-${{ matrix.name }}-${{ github.sha }}
          restore-keys: |
            kubler-${{ matrix.name }}-
            kubler-

      - name: Cache Kubler Gentoo Packages
        if: ${{ false }}
        uses: actions/cache@v4
        with:
          path: ~/.kubler/packages/
          key: kubler-${{ matrix.name }}-${{ github.sha }}
          restore-keys: |
            kubler-${{ matrix.name }}-
            kubler-

      - name: Azure login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Blob FUSE
        run: |
          set -eux
          sudo apt update
          sudo apt-get install -y blobfuse2
          ls -la /mnt
          # Get GitHub OAuth Token for OIDC
          curl -H "Authorization: Bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN" "$ACTIONS_ID_TOKEN_REQUEST_URL&audience=api://AzureADTokenExchange" | jq -r .value | sudo tee /root/token.txt
          cat <<-EOF > config.yaml
            # This allows all users access to the mount
            allow-other: true
            logging:
              level: log_debug
              file-path: "blobfuse2-logs.txt"
              type: base
            azstorage:
              account-name: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
              container: ${{ secrets.AZURE_STORAGE_CONTAINER }}
              # Doesn't work https://github.com/Azure/azure-storage-fuse/discussions/1449
              #mode: azcli
              mode: spn
              clientid: ${{ secrets.AZURE_CLIENT_ID }}
              tenantid: ${{ secrets.AZURE_TENANT_ID }}
              oauth-token-path: /root/token.txt
          EOF
          cat config.yaml
          sudo mkdir -p /mnt/blobfuse2
          sudo mkdir -p /mnt/blobfuse2tmp
          if sudo blobfuse2 mount /mnt/blobfuse2 --config-file=config.yaml --tmp-path=/mnt/blobfuse2tmp; then
            echo "DBG Blobfuse2 mount worked $?"
            ls -lad /mnt/blobfuse2
            ls -la /mnt/blobfuse2
            echo "Hello World" > /mnt/blobfuse2/hello.txt
            cat /mnt/blobfuse2/hello.txt
          else
            echo "ERR Blobfuse2 mount failed $?"
            ls -l blobfuse2-logs.txt
            cat blobfuse2-logs.txt
            exit 1
          fi

      - name: Symlink packages
        run: |
          ls -lad /mnt/blobfuse2
          ls -la /mnt/blobfuse2
          mkdir -p ~/.kubler
          ln -sf /mnt/blobfuse2 ~/.kubler/packages
          ls -lad ~/.kubler/packages
          ls -la ~/.kubler/packages

      # If too long between last run then caches might not exists anymore
      # So directories might not exist
      - name: Inspect Caches
        run: |
          set -eux
          ls -la /
          id
          pwd
          ls -la ~/.kubler || true
          ls -ltra ~/.kubler/downloads/ || true
          ls -ltra ~/.kubler/distfiles/ || true
          ls -ltra ~/.kubler/packages/ || true
          ls -ltra ~/.kubler/log/ || true

      - name: Inspect Distfiles Cache - Before
        continue-on-error: true
        run: |
          set -eux
          # Ensure distfiles exists
          mkdir -p ~/.kubler/distfiles
          ls -ltra ~/.kubler/distfiles/ | head -n 100

      - name: Inspect Packages Cache
        continue-on-error: true
        run: |
          set -eux
          # Ensure packages exists
          mkdir -p ~/.kubler/packages
          ls -ltra ~/.kubler/packages/
          ls -ltra ~/.kubler/packages/*
          tree -aC --filelimit 100 ~/.kubler/packages

      - name: Set up QEMU
        id: qemu
        uses: docker/setup-qemu-action@v3
        with:
          image: tonistiigi/binfmt:latest
          platforms: all

      - name: üêã Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v3
        with:
          # This breaks kubler https://github.com/edannenberg/kubler/issues/215
          # Sets up `docker build` command as an alias to `docker buildx` (default `false`)
          install: true

      - name: üêã Docker Login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # When we use `docker buildx` and `docker bake` we want to be able to access images we built in prior steps
      # For this we need the containerd-snapshotter feature which is in beta
      - name: Docker Daemon Config - Enable Containerd Snapshotter
        run: |
          id -a
          ls -l /etc/docker/daemon.json
          # Old config
          jq . /etc/docker/daemon.json
          # First read old config before we clobber it
          CONFIG=$(jq '.features["containerd-snapshotter"] = true' /etc/docker/daemon.json)
          # Write new config - sudo tee to write to root owned file
          echo "$CONFIG" | sudo tee /etc/docker/daemon.json
          jq . /etc/docker/daemon.json
          systemctl status docker
          sudo systemctl restart docker
          systemctl status docker
          docker run --rm hello-world

      - name: üêã Docker Buildx Inspect Again
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      - name: üîë Get Gentoo Portage GPG Key - Web Key Directory (WKD) Method
        id: gpg-wkd
        #if: ${{ steps.gpg-recv-key.outcome == 'failure' }}
        continue-on-error: true
        run: |
          gpg --auto-key-locate=clear,nodefault,wkd --locate-key releng@gentoo.org

      - name: üîë Get Gentoo Portage GPG Key - curl qa-reports Method
        id: gpg-web
        #if: ${{ steps.gpg-recv-key.outcome == 'failure' && steps.gpg-wkd.outcome == 'failure' }}
        if: ${{ steps.gpg-wkd.outcome == 'failure' }}
        continue-on-error: true
        run: |
          # keys.gentoo.org seems flakey lately
          curl -L https://qa-reports.gentoo.org/output/service-keys.gpg | gpg --import

      # This flakey step is taking 2mins to fail, so moved it to last resort
      - name: üîë Get Gentoo Portage GPG Key
        id: gpg-recv-key
        if: ${{ steps.gpg-wkd.outcome == 'failure' && steps.gpg-web.outcome == 'failure' }}
        # keys.gentoo.org seems flakey lately - alternative methods in steps below
        #continue-on-error: true
        run: |
          # For Portage signatures
          #
          # Fingerprint with spaces `gpg -k --fingerprint --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       Key fingerprint = DCD0 5B71 EAB9 4199 527F  44AC DB6B 8C1F 96D8 BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             Key fingerprint = E1D6 ABB6 3BFC FB4B A02F  DF1C EC59 0EEA C918 9250
          #
          # Fingerprint longkeyid no spaces `gpg -k --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250
          gpg --keyserver hkps://keys.gentoo.org --recv-keys DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D

          # For Stage3 signatures
          #
          # Fingerprint with spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       Key fingerprint = 13EB BDBE DE7A 1277 5DFD  B1BA BB57 2E0E 2D18 2910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             Key fingerprint = 534E 4209 AB49 EEE1 C19D  9616 2C44 695D B9F6 043D
          #
          # Fingerprint no spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       13EBBDBEDE7A12775DFDB1BABB572E0E2D182910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             534E4209AB49EEE1C19D96162C44695DB9F6043D
          #
          gpg --keyserver hkps://keys.gentoo.org --recv-keys 13EBBDBEDE7A12775DFDB1BABB572E0E2D182910

      - name: üîë List Gentoo Portage GPG Keys
        run: |
          gpg --list-public-keys
          gpg --list-public-keys --with-subkey-fingerprint
          gpg --list-public-keys --with-subkey-fingerprint --fingerprint
          # we just need the key, we don't need to sign/trust it

      - name: üë∑ Kubler Set kubler.conf PORTAGE_DATE
        run: |
          grep PORTAGE_DATE kubler.conf || true
          echo "PORTAGE_DATE=$PORTAGE_DATE" >> kubler.conf

      - name: üë∑ Kubler Set kubler.conf IMAGE_TAG
        run: |
          grep IMAGE_TAG kubler.conf || true
          echo "IMAGE_TAG=$PORTAGE_DATE" >> kubler.conf

      - name: üë∑ Check Kubler Downloads
        run: |
          ls -l ~/.kubler/downloads/portage* || true

      - name: üîë Check GPG
        run: |
          gpg --list-public-keys
          ls -l ~/.kubler/downloads/portage-"${PORTAGE_DATE}".* || true
          if [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ] && [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz ]; then
            gpg --verify ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz
          else
            echo "[!] No files to verify"
          fi

      - name: üêãüç≥ Docker Bake Kubler - images before baking
        run: |
          docker images

      - name: Check env before exposing GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env before exposing GitHub Runtime - full
        run: |
          env

      # Needed for bake-action to use GHA cache
      # XXX I think this may be a bit dangerous, exposing secrets to everything in the job
      - name: Expose GitHub Runtime
        #if: ${{ false }}
        uses: crazy-max/ghaction-github-runtime@v3

      - name: Check env after exposed GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env after exposed GitHub Runtime - full
        run: |
          env

      - name: üêã Docker Pull Portage
        id: portage-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$PORTAGE_IMAGE"
          docker tag "${PORTAGE_IMAGE}" kubler-gentoo/portage:latest

      - name: üêã Docker Pull stage3
        id: stage3-pull
        if: ${{ false }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_IMAGE"

      - name: üêã Docker Pull stage3 musl
        id: stage3-musl-pull
        if: ${{ false }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_MUSL_IMAGE"

      - name: üêã Docker Pull bob-core
        id: bob-core-pull
        if: ${{ matrix.bob }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_CORE_IMAGE"
          docker tag "${BOB_CORE_IMAGE}" kubler/bob-core:"${PORTAGE_DATE}"

      - name: üêã Docker Pull bob-musl-core
        id: bob-musl-core-pull
        if: ${{ matrix.bob-musl }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_CORE_IMAGE"
          docker tag "${BOB_MUSL_CORE_IMAGE}" kubler/bob-musl-core:"${PORTAGE_DATE}"

      - name: üêã Docker Pull bob
        id: bob-pull
        if: ${{ matrix.bob }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_IMAGE"
          docker tag "${BOB_IMAGE}" kubler/bob:"${PORTAGE_DATE}"

      - name: üêã Docker Pull bob-musl
        id: bob-musl-pull
        if: ${{ matrix.bob-musl }}
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_IMAGE"
          docker tag "${BOB_MUSL_IMAGE}" kubler/bob-musl:"${PORTAGE_DATE}"

      - name: üêãüç≥ Docker Images
        if: ${{ false }}
        run: |
          docker images

      - name: üêãüç≥ Docker Tag Images for Kubler Compatability
        if: ${{ false }}
        run: |
          # We need a local copy to tag
          # We build tmp image and use oras to copy it, so we don't have it locally
          #docker pull "${PORTAGE_IMAGE}"
          #docker pull "${BOB_CORE_IMAGE}"
          #docker pull "${BOB_MUSL_CORE_IMAGE}"
          #docker pull "${BOB_IMAGE}"
          #docker pull "${BOB_MUSL_IMAGE}"
          docker images

      - name: üêãüç≥ Docker Images
        run: |
          docker images

      - name: üë∑ Kubler Set kubler.conf DEFAULT_MUSL_BUILDER
        #if: ${{ false }}
        run: |
          # If we don't set the default musl builder, some images will fallback to using bob (glibc) builder
          grep DEFAULT_MUSL_BUILDER kubler.conf || true
          #echo "DEFAULT_MUSL_BUILDER=$BOB_MUSL_IMAGE" >> kubler.conf
          # Kubler doesn't understand fully qualified names
          # Gets error `fatal: Couldn't read namespace conf /kubler.conf`
          # We have retagged images to have compat names
          echo "DEFAULT_MUSL_BUILDER=kubler/bob-musl" >> kubler.conf

      - name: üë∑ Portage Container - eix-update - Populate /var/cache/eix
        if: ${{ !matrix.bob-musl && matrix.bob }}
        run: |
          set -u
          # Kubler building builders would use volume from portage container
          # and run `eix-update` which would populate /var/cache/eix.
          # But I'm building the builders directly with docker
          # Docker build doesn't support mounting volumes during builds
          # When using `kubler build` to build images with our builder,
          # the builds fail with this error:
          #
          #   cannot open database file /var/cache/eix/portage.eix for reading
          #
          # So this step will fix this to make kubler build for images happy.
          # kubler does `docker run`, but I think `docker create` is more idiomatic
          docker create --name kubler-gentoo-portage "${PORTAGE_IMAGE}" true
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_IMAGE}" ls -l /var/cache/eix
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_IMAGE}" eix-update
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_IMAGE}" ls -l /var/cache/eix

      - name: üë∑ Portage Container - eix-update - Populate /var/cache/eix - musl
        if: ${{ matrix.bob-musl }}
        run: |
          set -u
          # Kubler building builders would use volume from portage container
          # and run `eix-update` which would populate /var/cache/eix.
          # But I'm building the builders directly with docker
          # Docker build doesn't support mounting volumes during builds
          # When using `kubler build` to build images with our builder,
          # the builds fail with this error:
          #
          #   cannot open database file /var/cache/eix/portage.eix for reading
          #
          # So this step will fix this to make kubler build for images happy.
          # kubler does `docker run`, but I think `docker create` is more idiomatic
          docker create --name kubler-gentoo-portage "${PORTAGE_IMAGE}" true
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" eix-update
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix

      - name: üöß Pull Kubler Image
        run: |
          docker pull "${KUBLER_IMAGE}"

      - name: Ensure ~/.kubler Directory Exists
        run: |
          mkdir -p ~/.kubler
          touch ~/.kubler/kubler.conf

      - name: üöß Kubler Build Image - Inspect
        #if: ${{ false }}
        #id: ${{ matrix.name }}
        # This is docker inception
        # We run the kubler container to run kubler
        # Kubler runs docker itself, but it will be mounting volumes from the host's POV
        run: |
          set -eux
          echo "HOME=$HOME"
          realpath ~/.kubler || true
          realpath ~/.kubler/downloads || true
          realpath ~/.kubler/distfiles || true
          realpath ~/.kubler/packages || true
          ls -la ~/.kubler || true
          export TERM
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v ~/.kubler:/$HOME/.kubler \
            -v "$(pwd)":"$(pwd)" \
            -w "$(pwd)" \
            -e TERM=dumb \
            "${KUBLER_IMAGE}" \
            sh -c 'set -eux; id; pwd; echo HOME=$HOME; ls -la; ls -la ~/.kubler; ls -la ~/.kubler/downloads || true; ls -la ~/.kubler/distfiles || true; ls -la ~/.kubler/packages || true; ls -la ~/.kubler/log || true; ls -la ~/.kubler/namespaces || true; ls -la ~/.kubler/namespaces/kubler || true; mount'

      - name: üöß Kubler Build Image
        #if: ${{ false }}
        #id: ${{ matrix.name }}
        id: kubler-build
        # We need `~/.kubler/namespaces/kubler` in the container to exist on the host,
        # so that the `kubler build` docker volume mounts are correct from host's POV.
        # Mounting `~/.kubler` from host should shadow that in the container.
        # `~/.kubler/namespaces` won't exist yet, so `kubler` should re-create it, and it will be on the host this time.
        run: |
          export TERM
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v ~/.kubler:/$HOME/.kubler \
            -v "$(pwd)":"$(pwd)" \
            -w "$(pwd)" \
            -e TERM=dumb \
            "${KUBLER_IMAGE}" \
            kubler build -v "${{ matrix.name }}"

      - name: Test Image
        #id: test-${{ matrix.name }}
        id: test
        if: ${{ matrix.goss }}
        # If the image fails, we want to push it under /bad/ so we can inspect it
        continue-on-error: true
        # The matrix might not define these
        env:
          CONTAINER_MODE: ${{ matrix.goss.container_mode }}
          CONTAINER_GOSS_PATH: ${{ matrix.goss.container_goss_path }}
          GOSS_FILES_STRATEGY: ${{ matrix.goss.goss_files_strategy }}
        # We use the env vars with defaults in case they were empty
        # Since we are doing docker inception it is easier to always use `cp` strategy than to try to get mounts in the container matching the host - since `dgoss` uses tmp files we'd have to use `DGOSS_TEMP_DIR` etc.
        run: |
          cd "${{ matrix.goss.dir }}" || false
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v "$(pwd):/src:ro" \
            -w /src \
            -e CONTAINER_MODE="${CONTAINER_MODE:-inject}" \
            -e CONTAINER_GOSS_PATH="${CONTAINER_GOSS_PATH:-/goss}" \
            -e GOSS_FILES_STRATEGY="${GOSS_FILES_STRATEGY:-cp}" \
            "$KUBLER_IMAGE" \
              dgoss run \
                ${{ matrix.goss.docker_args }} \
                "${{ matrix.image }}" ${{ matrix.goss.goss_args }}

      - name: üöß Build Images - push
        if: ${{ matrix.push }}
        #id: push-${{ matrix.name}}
        id: push
        run: |
          # The buildx docker-container driver can't pull from the local registry
          # So we need to push the image
          IMAGE="ghcr.io/berney/kubler-images/${{ matrix.name }}"
          docker tag ${{ matrix.image }} "$IMAGE"
          docker push "$IMAGE"

      - name: üöß Build Images - push bad image
        #if: ${{ matrix.push }}
        if: ${{ steps.test.outcome == 'failure' }}
        #id: push-${{ matrix.name}}
        id: push-bad
        run: |
          # The buildx docker-container driver can't pull from the local registry
          # So we need to push the image
          IMAGE="ghcr.io/berney/kubler-images/bad/${{ matrix.name }}"
          docker tag ${{ matrix.image }} "$IMAGE"
          docker push "$IMAGE"

      - name: üîé Post Build Inspect
        run: |
          set -eux
          docker images
          docker ps -a
          df -h
          git status
          git ls-files -o
          ls -ld ~/.kubler
          ls -la ~/.kubler
          ls -ld ~/.kubler/log || true
          ls -la ~/.kubler/log || true
          ls -l ~/.kubler/log/build.log || true
          cat ~/.kubler/log/build.log || true

      # Fail the job if the testing step above failed
      # Because the test step had `continue-on-error: true` it's conclusion will be 'success'
      # But we want the job to fail if testing fails
      - name: ‚ùì Outcome
        if: ${{ steps.test.outcome == 'failure' }}
        run: |
          # should fail the job
          false
