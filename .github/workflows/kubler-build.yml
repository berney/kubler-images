name: Kubler Build
on:
  push:
    branches:
      - main
  workflow_dispatch:
  workflow_call:
    inputs:
      portage_date:
        description: The date (tag) of the latest portage image (e.g. `20241231`)
        required: true
        # Could be `latest` or a date like `20241231`
        type: string
      portage_image:
        description: Portage Image (e.g. `ghcr.io/berney/kubler-images/portage:20241231`)
        required: true
        type: string

# Jobs run in parallel
# Jobs are independent with separate file systems, IP addresses, etc.
jobs:
  kubler:
    runs-on: ubuntu-latest
    env:
      KUBLER_IMAGE: ghcr.io/${{ github.repository }}
      PORTAGE_DATE: ${{ inputs.portage_date }}
      PORTAGE_IMAGE: ${{ inputs.portage_image }}
      STAGE3_IMAGE: ghcr.io/${{ github.repository }}/stage3-amd64-hardened-nomultilib-openrc:${{ inputs.portage_date }}
      STAGE3_MUSL_IMAGE: ghcr.io/${{ github.repository }}/stage3-amd64-musl-hardened:${{ inputs.portage_date }}
      BOB_CORE_IMAGE: ghcr.io/${{ github.repository }}/bob-core:${{ inputs.portage_date }}
      BOB_MUSL_CORE_IMAGE: ghcr.io/${{ github.repository }}/bob-musl-core:${{ inputs.portage_date }}
      BOB_IMAGE: ghcr.io/${{ github.repository }}/bob:${{ inputs.portage_date }}
      BOB_MUSL_IMAGE: ghcr.io/${{ github.repository }}/bob-musl:${{ inputs.portage_date }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: busybox
            image: kubler-images/busybox
            goss:
              dir: images/busybox
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: kubler/busybox
            image: kubler/busybox
            goss:
              dir: images/busybox
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: kubler/glibc
            image: kubler/glibc
            goss:
              dir: images/glibc
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet
            image: kubler-images/figlet
            goss:
              dir: images/figlet
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-user
            image: kubler-images/figlet-user
            goss:
              dir: images/figlet-user
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-musl
            image: kubler-images/figlet-musl
            goss:
              dir: images/figlet-musl
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: figlet-musl-static
            image: kubler-images/figlet-musl-static
            goss:
              dir: images/figlet-musl-static
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: goss
            image: kubler-images/goss
            goss:
              dir: images/goss
              container_mode: entrypoint
              # Because the image is already using `/goss` (and it's a file), we need `dgoss` to use a different directory
              # I've added the `CONTAINER_GOSS_PATH` env var to allow overriding the default
              container_goss_path: /goss2
              docker_args: -w /goss2 --entrypoint /goss2/goss
              goss_args: validate --color
          - name: fd
            image: kubler-images/fd
            goss:
              dir: images/fd
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: s6
            image: kubler-images/s6
            push: true
            goss:
              dir: images/s6
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: validate --color
          - name: s6-busybox
            image: kubler-images/s6-busybox
            push: true
            goss:
              dir: images/s6-busybox
              container_mode: inject
              # ash needs a terminal to start
              docker_args: -t
              goss_args:
          - name: coturn
            image: kubler-images/coturn
            #push: true
            #goss:
            #  dir: images/coturn
            #  container_mode: inject
            #  # ash needs a terminal to start
            #  docker_args: -t
            #  goss_args:
          - name: s6-coturn-busybox
            image: kubler-images/s6-coturn-busybox
            #push: true
            #goss:
            #  dir: images/coturn
            #  container_mode: inject
            #  # ash needs a terminal to start
            #  docker_args: -t
            #  goss_args:
          - name: nmap
            image: kubler-images/nmap
            #push: true
            # extra test: docker run --rm kubler-images/nmap -A --open scanme.nmap.org
            goss:
              dir: images/nmap
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: -l debug validate --color
          - name: nmap-musl-static
            image: kubler-images/nmap-musl-static
            #push: true
            # extra test: docker run --rm kubler-images/nmap-musl-static -A --open scanme.nmap.org
            goss:
              dir: images/nmap-musl-static
              container_mode: entrypoint
              docker_args: -w /goss --entrypoint /goss/goss
              goss_args: -l debug validate --color
          - name: tmux
            image: kubler-images/tmux
            #goss:
            #  dir: images/tmux
            #  container_mode: entrypoint
            #  docker_args: -w /goss --entrypoint /goss/goss
            #  goss_args: validate --color

    steps:
      - name: Inspect
        run: |
          set -eux
          id
          uname
          pwd
          echo "${{ github.workspace }}"
          echo $HOME
          ls -la
          ls -la /
          cd
          pwd
          mount

      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Update Docker Engine
        # Disable
        if: ${{ false }}
        run: |
          docker version
          docker info
          # Remove old packages
          for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg || true; done
          sudo apt-get update
          sudo apt-get install ca-certificates curl gnupg
          ls -ld /etc/apt/keyrings || true
          ls -l /etc/apt/keyrings/docker.gpg || true
          sudo install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          sudo chmod a+r /etc/apt/keyrings/docker.gpg
          ls -ld /etc/apt/sources.list.d || true
          ls -l /etc/apt/sources.list.d/ || true
          ls -l /etc/apt/sources.list.d/docker.list || true
          echo \
            "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
            "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
            sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
          cat /etc/apt/sources.list.d/docker.list
          sudo apt-get update
          sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
          systemctl status docker
          sudo systemctl start docker
          docker version
          docker info
          docker run --rm hello-world

      # Cache Versions are based off key and path, so differnt path's can use same key
      #
      # Caches are immutable, so need unique key to create a new cache
      # `restore-keys` provides a list to restore a cache when key doesn't match
      # If there's no exact match, the most recent cache that partially matches will be used
      #
      - name: Cache Kubler Downloads
        uses: actions/cache@v4
        with:
          path: ~/.kubler/downloads/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Distfiles
        uses: actions/cache@v4
        with:
          path: ~/.kubler/distfiles/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Packages
        uses: actions/cache@v4
        with:
          path: ~/.kubler/packages/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Inspect Caches
        run: |
          set -eux
          ls -la /
          id
          pwd
          ls -la ~/.kubler
          ls -ltra ~/.kubler/downloads/
          ls -ltra ~/.kubler/distfiles/
          ls -ltra ~/.kubler/packages/
          ls -ltra ~/.kubler/log/ || true

      - name: Set up QEMU
        id: qemu
        uses: docker/setup-qemu-action@v3
        with:
          image: tonistiigi/binfmt:latest
          platforms: all

      - name: üêã Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v3
        with:
          # This breaks kubler https://github.com/edannenberg/kubler/issues/215
          # Sets up `docker build` command as an alias to `docker buildx` (default `false`)
          install: true

      - name: üêã Docker Login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # When we use `docker buildx` and `docker bake` we want to be able to access images we built in prior steps
      # For this we need the containerd-snapshotter feature which is in beta
      - name: Docker Daemon Config - Enable Containerd Snapshotter
        run: |
          id -a
          ls -l /etc/docker/daemon.json
          # Old config
          jq . /etc/docker/daemon.json
          # First read old config before we clobber it
          CONFIG=$(jq '.features["containerd-snapshotter"] = true' /etc/docker/daemon.json)
          # Write new config - sudo tee to write to root owned file
          echo "$CONFIG" | sudo tee /etc/docker/daemon.json
          jq . /etc/docker/daemon.json
          systemctl status docker
          sudo systemctl restart docker
          systemctl status docker
          docker run --rm hello-world

      - name: üêã Docker Buildx Inspect Again
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      - name: üîë Get Gentoo Portage GPG Key
        run: |
          # For Portage signatures
          #
          # Fingerprint with spaces `gpg -k --fingerprint --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       Key fingerprint = DCD0 5B71 EAB9 4199 527F  44AC DB6B 8C1F 96D8 BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             Key fingerprint = E1D6 ABB6 3BFC FB4B A02F  DF1C EC59 0EEA C918 9250
          #
          # Fingerprint longkeyid no spaces `gpg -k --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250
          gpg --keyserver keys.gentoo.org --recv-keys DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D

          # For Stage3 signatures
          #
          # Fingerprint with spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       Key fingerprint = 13EB BDBE DE7A 1277 5DFD  B1BA BB57 2E0E 2D18 2910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             Key fingerprint = 534E 4209 AB49 EEE1 C19D  9616 2C44 695D B9F6 043D
          #
          # Fingerprint no spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       13EBBDBEDE7A12775DFDB1BABB572E0E2D182910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             534E4209AB49EEE1C19D96162C44695DB9F6043D
          #
          gpg --keyserver keys.gentoo.org --recv-keys 13EBBDBEDE7A12775DFDB1BABB572E0E2D182910

          gpg --list-public-keys
          gpg --list-public-keys --with-subkey-fingerprint
          gpg --list-public-keys --with-subkey-fingerprint --fingerprint
          # we just need the key, we don't need to sign/trust it

      - name: üë∑ Kubler Set kubler.conf PORTAGE_DATE
        run: |
          grep PORTAGE_DATE kubler.conf || true
          echo "PORTAGE_DATE=$PORTAGE_DATE" >> kubler.conf

      - name: üë∑ Kubler Set kubler.conf IMAGE_TAG
        run: |
          grep IMAGE_TAG kubler.conf || true
          echo "IMAGE_TAG=$PORTAGE_DATE" >> kubler.conf

      - name: üë∑ Check Kubler Downloads
        run: |
          ls -l ~/.kubler/downloads/portage* || true

      - name: üîë Check GPG
        run: |
          gpg --list-public-keys
          ls -l ~/.kubler/downloads/portage-"${PORTAGE_DATE}".* || true
          if [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ] && [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz ]; then
            gpg --verify ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz
          else
            echo "[!] No files to verify"
          fi

      - name: üêãüç≥ Docker Bake Kubler - images before baking
        run: |
          docker images

      - name: Check env before exposing GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env before exposing GitHub Runtime - full
        run: |
          env

      # Needed for bake-action to use GHA cache
      # XXX I think this may be a bit dangerous, exposing secrets to everything in the job
      - name: Expose GitHub Runtime
        #if: ${{ false }}
        uses: crazy-max/ghaction-github-runtime@v3

      - name: Check env after exposed GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env after exposed GitHub Runtime - full
        run: |
          env

      - name: üêã Docker Pull Portage
        id: portage-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$PORTAGE_IMAGE"

      - name: üêã Docker Pull stage3
        id: stage3-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_IMAGE"

      - name: üêã Docker Pull stage3 musl
        id: stage3-musl-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_MUSL_IMAGE"

      - name: üêã Docker Pull bob-core
        id: bob-core-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_CORE_IMAGE"

      - name: üêã Docker Pull bob-musl-core
        id: bob-musl-core-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_CORE_IMAGE"

      - name: üêã Docker Pull bob
        id: bob-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_IMAGE"

      - name: üêã Docker Pull bob-musl
        id: bob-musl-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_IMAGE"

      - name: üêãüç≥ Docker Images
        run: |
          docker images

      - name: üêãüç≥ Docker Tag Images for Kubler Compatability
        run: |
          # We need a local copy to tag
          # We build tmp image and use oras to copy it, so we don't have it locally
          docker pull "${PORTAGE_IMAGE}"
          docker pull "${BOB_CORE_IMAGE}"
          docker pull "${BOB_MUSL_CORE_IMAGE}"
          docker pull "${BOB_IMAGE}"
          docker pull "${BOB_MUSL_IMAGE}"
          docker tag "${PORTAGE_IMAGE}" kubler-gentoo/portage:latest
          docker tag "${BOB_CORE_IMAGE}" kubler/bob-core:"${PORTAGE_DATE}"
          docker tag "${BOB_MUSL_CORE_IMAGE}" kubler/bob-musl-core:"${PORTAGE_DATE}"
          docker tag "${BOB_IMAGE}" kubler/bob:"${PORTAGE_DATE}"
          docker tag "${BOB_MUSL_IMAGE}" kubler/bob-musl:"${PORTAGE_DATE}"
          docker images

      - name: üêãüç≥ Docker Images
        run: |
          docker images

      - name: üë∑ Kubler Set kubler.conf DEFAULT_MUSL_BUILDER
        #if: ${{ false }}
        run: |
          # If we don't set the default musl builder, some images will fallback to using bob (glibc) builder
          grep DEFAULT_MUSL_BUILDER kubler.conf || true
          #echo "DEFAULT_MUSL_BUILDER=$BOB_MUSL_IMAGE" >> kubler.conf
          # Kubler doesn't understand fully qualified names
          # Gets error `fatal: Couldn't read namespace conf /kubler.conf`
          # We have retagged images to have compat names
          echo "DEFAULT_MUSL_BUILDER=kubler/bob-musl" >> kubler.conf

      - name: üë∑ Portage Container - eix-update - Populate /var/cache/eix
        run: |
          set -u
          # Kubler building builders would use volume from portage container
          # and run `eix-update` which would populate /var/cache/eix.
          # But I'm building the builders directly with docker
          # Docker build doesn't support mounting volumes during builds
          # When using `kubler build` to build images with our builder,
          # the builds fail with this error:
          #
          #   cannot open database file /var/cache/eix/portage.eix for reading
          #
          # So this step will fix this to make kubler build for images happy.
          # kubler does `docker run`, but I think `docker create` is more idiomatic
          docker create --name kubler-gentoo-portage "${PORTAGE_IMAGE}" true
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" eix-update
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix

      - name: üöß Pull Kubler Image
        run: |
          docker pull "${KUBLER_IMAGE}"

      - name: üöß Kubler Build Image - Inspect
        #if: ${{ false }}
        #id: ${{ matrix.name }}
        # This is docker inception
        # We run the kubler container to run kubler
        # Kubler runs docker itself, but it will be mounting volumes from the host's POV
        run: |
          set -eux
          echo "kubler=build" >> $GITHUB_OUTPUT
          echo "HOME=$HOME"
          realpath ~/.kubler
          realpath ~/.kubler/downloads
          realpath ~/.kubler/distfiles
          realpath ~/.kubler/packages
          ls -la ~/.kubler
          touch ~/.kubler/kubler.conf
          export TERM
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v ~/.kubler:/$HOME/.kubler \
            -v "$(pwd)":"$(pwd)" \
            -w "$(pwd)" \
            -e TERM=dumb \
            "${KUBLER_IMAGE}" \
            sh -c 'set -eux; id; pwd; echo HOME=$HOME; ls -la; ls -la ~/.kubler; ls -la ~/.kubler/downloads; ls -la ~/.kubler/distfiles; ls -la ~/.kubler/packages; ls -la ~/.kubler/log || true; ls -la ~/.kubler/namespaces || true; ls -la ~/.kubler/namespaces/kubler || true; mount'

      - name: üöß Kubler Build Image
        #if: ${{ false }}
        #id: ${{ matrix.name }}
        id: kubler-build
        # We need `~/.kubler/namespaces/kubler` in the container to exist on the host,
        # so that the `kubler build` docker volume mounts are correct from host's POV.
        # Mounting `~/.kubler` from host should shadow that in the container.
        # `~/.kubler/namespaces` won't exist yet, so `kubler` should re-create it, and it will be on the host this time.
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v ~/.kubler:/$HOME/.kubler \
            -v "$(pwd)":"$(pwd)" \
            -w "$(pwd)" \
            -e TERM=dumb \
            "${KUBLER_IMAGE}" \
            kubler build -v "${{ matrix.name }}"

      - name: Test Image
        #id: test-${{ matrix.name }}
        id: test
        if: ${{ matrix.goss }}
        # The matrix might not define these
        env:
          CONTAINER_MODE: ${{ matrix.goss.container_mode }}
          CONTAINER_GOSS_PATH: ${{ matrix.goss.container_goss_path }}
          GOSS_FILES_STRATEGY: ${{ matrix.goss.goss_files_strategy }}
        # We use the env vars with defaults in case they were empty
        # Since we are doing docker inception it is easier to always use `cp` strategy than to try to get mounts in the container matching the host - since `dgoss` uses tmp files we'd have to use `DGOSS_TEMP_DIR` etc.
        run: |
          cd "${{ matrix.goss.dir }}" || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          docker run \
            --rm \
            -v /run/docker.sock:/run/docker.sock \
            -v "$(pwd):/src:ro" \
            -w /src \
            -e CONTAINER_MODE="${CONTAINER_MODE:-inject}" \
            -e CONTAINER_GOSS_PATH="${CONTAINER_GOSS_PATH:-/goss}" \
            -e GOSS_FILES_STRATEGY="${GOSS_FILES_STRATEGY:-cp}" \
            "$KUBLER_IMAGE" \
              dgoss run \
                ${{ matrix.goss.docker_args }} \
                "${{ matrix.image }}" ${{ matrix.goss.goss_args }}

      - name: üöß Build Images - push
        if: ${{ matrix.push }}
        #id: push-${{ matrix.name}}
        id: push
        run: |
          # The buildx docker-container driver can't pull from the local registry
          # So we need to push the image
          docker tag ${{ matrix.image }} ghcr.io/berney/kubler-images/${{ matrix.name }}
          docker push ghcr.io/berney/kubler-images/${{ matrix.name }}

      - name: üöß Build Images - Everything excluding Problematic
        if: ${{ false }}
        id: everything-exc-problematic
        continue-on-error: true
        run: |
          #echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build kubler-images -e kubler-images/nmap-musl-static

      - name: üöß Build Images - EVERYTHING
        if: ${{ false }}
        id: everything
        continue-on-error: true
        run: |
          #echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build kubler-images

      - name: üîé Post Build Inspect
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          set -eux
          docker images
          docker ps -a
          df -h
          git status
          git ls-files -o
          ls -ld ~/.kubler
          ls -la ~/.kubler
          ls -ld ~/.kubler/log || true
          ls -la ~/.kubler/log || true
          ls -l ~/.kubler/log/build.log || true
          cat ~/.kubler/log/build.log || true
          echo "== Steps"
          printf '%s\n' "$STEPS"
          echo "== Steps | jq"
          printf '%s\n' "$STEPS" | jq .
          echo "== Steps | jq expr"
          printf '%s\n' "$STEPS" | jq 'to_entries | map(select(.value.outputs.kubler == "build").key)'

      - name: üîé Sumarise Build Status
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          echo "# Successful Build Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")'
          echo "# Failed Build Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")'

          echo "# Successful Build Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY
          echo "# Failed Build Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY

      - name: üîé Sumarise Test Status
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          echo "# Successful Test Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome == "success").key | sub("^"; "* ")) | join("\n")'
          echo "# Failed Test Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome != "success").key | sub("^"; "* ")) | join("\n")'

          echo "# Successful Test Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome == "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY
          echo "# Failed Test Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome != "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY

      - run: echo "üçè This job's status is ${{ job.status }}."
