name: Kubler Build
on:
  push:
    branches:
      - main
  workflow_dispatch:
jobs:
  kubler:
    runs-on: ubuntu-latest
    steps:
      - run: echo "üéâ The job was automatically triggered by a ${{ github.event_name }} event."
      - run: echo "üêß This job is now running on a ${{ runner.os }} server hosted by GitHub!"
      - run: echo "üîé The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}."

      - name: Check out repository code
        uses: actions/checkout@v4

      - run: echo "üí° The ${{ github.repository }} repository has been cloned to the runner."
      - run: echo "üñ•Ô∏è The workflow is now ready to test your code on the runner."

      - name: List files in the repository
        run: |
          ls ${{ github.workspace }}

      - name: üîé Inspect Runner
        run: |
          df -h
          free -m
          bash --version
          docker version
          docker info
          pwd
          echo $PATH

      - name: Update Docker Engine
        run: |
          docker version
          docker info
          # Remove old packages
          for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg || true; done
          sudo apt-get update
          sudo apt-get install ca-certificates curl gnupg
          ls -ld /etc/apt/keyrings || true
          ls -l /etc/apt/keyrings/docker.gpg || true
          sudo install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          sudo chmod a+r /etc/apt/keyrings/docker.gpg
          ls -ld /etc/apt/sources.list.d || true
          ls -l /etc/apt/sources.list.d/ || true
          ls -l /etc/apt/sources.list.d/docker.list || true
          echo \
            "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
            "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
            sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
          cat /etc/apt/sources.list.d/docker.list
          sudo apt-get update
          sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
          systemctl status docker
          sudo systemctl start docker
          docker version
          docker info
          docker run --rm hello-world

      - name: üë∑ Install Kubler
        run: |
          cd ..

          ## Install from tarball
          #
          #curl -L https://github.com/edannenberg/kubler/archive/master.tar.gz | tar xz
          #ls -ld kubler-master
          #mv kubler-master kubler
          #ls -l kubler/bin
          #echo "$(pwd)/kubler/bin" >> $GITHUB_PATH

          ## Install from Git
          #
          git clone https://github.com/berney/kubler.git
          cd kubler
          # Using my branch to pick-up fixes until PRs are merged
          #git checkout f-berne
          git checkout f-experiment-buildx-bake
          #git checkout f-berne-bake
          ls -l bin
          echo "$(pwd)/bin" >> $GITHUB_PATH

      - name: üë∑ Kubler Version
        run: |
          export TERM
          kubler --help

      - name: üë∑ Kubler Inspect Terminal
        run: |
          echo "TERM=$TERM"
          echo $PATH
          pwd
          env
          # test term colors
          kubler dep-graph xxx || true
          echo "export TERM"
          export TERM
          kubler dep-graph xxx || true
          echo "export TERM=dumb"
          export TERM
          export TERM=dumb
          kubler dep-graph xxx || true
          echo "export TERM=xterm"
          export TERM=xterm
          kubler dep-graph xxx || true

      # Cache Versions are based off key and path, so differnt path's can use same key
      #
      # Caches are immutable, so need unique key to create a new cache
      # `restore-keys` provides a list to restore a cache when key doesn't match
      # If there's no exact match, the most recent cache that partially matches will be used
      #
      - name: Cache Kubler Downloads
        uses: actions/cache@v3
        with:
          path: ~/.kubler/downloads/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Distfiles
        uses: actions/cache@v3
        with:
          path: ~/.kubler/distfiles/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Packages
        uses: actions/cache@v3
        with:
          path: ~/.kubler/packages/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Set up QEMU
        id: qemu
        uses: docker/setup-qemu-action@v3
        with:
          image: tonistiigi/binfmt:latest
          platforms: all

      - name: üêã Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v3
        with:
          # This breaks kubler https://github.com/edannenberg/kubler/issues/215
          # Sets up `docker build` command as an alias to `docker buildx` (default `false`)
          install: true

      - name: üêã Inspect builder
        run: |
          echo "Name:      ${{ steps.buildx.outputs.name }}"
          echo "Endpoint:  ${{ steps.buildx.outputs.endpoint }}"
          echo "Status:    ${{ steps.buildx.outputs.status }}"
          echo "Flags:     ${{ steps.buildx.outputs.flags }}"
          echo "Platforms: ${{ steps.buildx.outputs.platforms }}"

      - name: üêã Docker Login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
          # this is the default, end of job will logout
          #logout: true

      - name: üêã Docker Buildx Inspect
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      # When we use `docker buildx` and `docker bake` we want to be able to access images we built in prior steps
      # For this we need the containerd-snapshotter feature which is in beta
      - name: Docker Daemon Config - Enable Containerd Snapshotter
        run: |
          id -a
          ls -l /etc/docker/daemon.json
          # Old config
          jq . /etc/docker/daemon.json
          # First read old config before we clobber it
          CONFIG=$(jq '.features["containerd-snapshotter"] = true' /etc/docker/daemon.json)
          # Write new config - sudo tee to write to root owned file
          echo "$CONFIG" | sudo tee /etc/docker/daemon.json
          jq . /etc/docker/daemon.json
          systemctl status docker
          sudo systemctl restart docker
          systemctl status docker
          docker run --rm hello-world

      - name: üêã Docker Buildx Inspect Again
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      - name: üë∑ Update Kubler (kubler-images, Gentoo Stage3)
        run: |
          export TERM
          ls -la ~/.kubler || true
          ls -la ~/.kubler/namespaces || true
          ls -la ~/.kubler/namespaces/kubler || true
          kubler update
          ls -la ~/.kubler || true
          ls -la ~/.kubler/namespaces || true
          ls -la ~/.kubler/namespaces/kubler || true

      - name: üë∑ Inspect Kubler Images
        run: |
          cd ~/.kubler/namespaces/kubler/
          git remote -v
          git status
          git ls-files -o
          git diff

      - name: üë∑ Fix Kubler Images bob-musl
        # Disable
        if: ${{ false }}
        run: |
          cd ~/.kubler/namespaces/kubler/
          git remote -v
          git remote add berney https://github.com/berney/kubler-images-1.git
          git fetch --all
          git checkout berney/b-fix-util-linux-su-pam
          git diff master..HEAD

      - name: üë∑ Fix Kubler Images busybox
        # Disable
        if: ${{ false }}
        run: |
          cd ~/.kubler/namespaces/kubler/
          git remote -v
          git remote add berney https://github.com/berney/kubler-images-1.git
          git fetch --all
          git checkout berney/f-berne
          git diff master..HEAD

      - name: üë∑ Add Custom Kubler Command(s)
        run: |
          echo "Kubler Commands (OG)"
          pwd
          ls -l
          tree -a -C ../kubler
          ls -l ../kubler
          ls -l ../kubler/cmd
          echo "Custom Kubler Commands"
          ls -l kubler/cmd
          rsync -avi kubler/cmd/* ../kubler/cmd/
          echo "Kubler Commands (now)"
          ls -l ../kubler/cmd

      - name: üîë Get Gentoo Portage GPG Key
        run: |
          # For Portage signatures
          #
          # Fingerprint with spaces `gpg -k --fingerprint --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       Key fingerprint = DCD0 5B71 EAB9 4199 527F  44AC DB6B 8C1F 96D8 BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             Key fingerprint = E1D6 ABB6 3BFC FB4B A02F  DF1C EC59 0EEA C918 9250
          #
          # Fingerprint longkeyid no spaces `gpg -k --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250
          gpg --keyserver keys.gentoo.org --recv-keys DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D

          # For Stage3 signatures
          #
          # Fingerprint with spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       Key fingerprint = 13EB BDBE DE7A 1277 5DFD  B1BA BB57 2E0E 2D18 2910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             Key fingerprint = 534E 4209 AB49 EEE1 C19D  9616 2C44 695D B9F6 043D
          #
          # Fingerprint no spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       13EBBDBEDE7A12775DFDB1BABB572E0E2D182910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             534E4209AB49EEE1C19D96162C44695DB9F6043D
          #
          gpg --keyserver keys.gentoo.org --recv-keys 13EBBDBEDE7A12775DFDB1BABB572E0E2D182910

          gpg --list-public-keys
          gpg --list-public-keys --with-subkey-fingerprint
          gpg --list-public-keys --with-subkey-fingerprint --fingerprint
          # we just need the key, we don't need to sign/trust it

      - name: üë∑ Kubler Get Latest Portage Date
        run: |
          export TERM
          PORTAGE_DATE=$(kubler portage)
          echo "PORTAGE_DATE=$PORTAGE_DATE"
          echo "PORTAGE_DATE=$PORTAGE_DATE" >> $GITHUB_ENV

      - name: üë∑ Check Kubler Downloads
        run: |
          ls -l ~/.kubler/downloads/portage* || true

      - name: üîë Check GPG
        run: |
          gpg --list-public-keys
          ls -l ~/.kubler/downloads/portage-"${PORTAGE_DATE}".* || true
          if [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ] && [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz ]; then
            gpg --verify ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz
          else
            echo "[!] No files to verify"
          fi

      - name: üêãüç≥ Docker Bake Kubler - images before baking
        run: |
          pwd
          docker images

      - name: üêãüç≥ Docker Bake Kubler portage
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-portage || exit 1
          # Repeating `--set` adds to the array
          # - https://github.com/docker/buildx/issues/872
          # - Doing it this way results in one image (repository hash) with two tags
          # - doing separate `TAG=xxx docker buildx bake` commands results in two different images (repository hashes), with identical layers, and near identical metadata.
          docker buildx bake --load --set kubler-portage.tags=kubler-gentoo/portage:latest --set kubler-portage.tags=kubler-gentoo/portage:buildx2

      - name: üêãüç≥ Docker Bake Kubler - images portage
        run: |
          pwd
          docker images
          docker run --rm kubler-gentoo/portage grep TIMESTAMP /var/db/repos/gentoo/Manifest

      - name: üêãüç≥ Docker Bake Kubler stage3
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-stage3 || exit 1
          BASE_TAG=musl-hardened docker buildx bake --load --set gentoo-stage3.tags=kubler-gentoo/stage3-amd64-musl-hardened:latest --set gentoo-stage3.tags=kubler-gentoo/stage3-amd64-musl-hardened:20230423T164653Z
          BASE_TAG=hardened-nomultilib docker buildx bake --load --set gentoo-stage3.tags=kubler-gentoo/stage3-amd64-hardened-nomultilib-openrc:latest --set gentoo-stage3.tags=kubler-gentoo/stage3-amd64-hardened-nomultilib-openrc:20230423T164653Z

      - name: üêãüç≥ Docker Bake Kubler - images stage3
        run: |
          pwd
          docker images
          docker run --rm kubler-gentoo/stage3-amd64-musl-hardened cat /etc/gentoo-release
          docker run --rm kubler-gentoo/stage3-amd64-hardened-nomultilib-openrc cat /etc/gentoo-release

      - name: üêãüç≥ Docker Bake Kubler bob-core
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-core || exit 1
          DEF_CHOST=x86_64-gentoo-linux-musl BASE_IMAGE=kubler-gentoo/stage3-amd64-musl-hardened docker buildx bake --load --set core.tags=kubler/bob-musl-core:latest --set core.tags=kubler/bob-musl-core:buildx2
          BASE_IMAGE=kubler-gentoo/stage3-amd64-hardened-nomultilib-openrc docker buildx bake --load --set core.tags=kubler/bob-core:latest --set core.tags=kubler/bob-core:buildx2
          #docker images | grep kubler

      - name: üêãüç≥ Docker Bake Kubler - images bob-core
        run: |
          pwd
          docker images


      - name: üöß Build Images - kubler/busybox
        #if: ${{ false }}
        id: busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          echo "$PORTAGE_DATE"
          kubler build -v kubler/busybox

      - name: üöß Build Images - kubler/glibc
        #if: ${{ false }}
        id: glibc
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v kubler/glibc

      - name: üöß Build Images - figlet
        #if: ${{ false }}
        id: figlet
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet

      - name: üöß Build Images - figlet-user
        #if: ${{ false }}
        id: figlet-user
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-user

      - name: üöß Build Images - figlet-musl
        #if: ${{ false }}
        id: figlet-musl
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-musl

      - name: üöß Build Images - figlet-musl-static
        #if: ${{ false }}
        id: figlet-musl-static
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-musl-static

      - name: üöß Build Images - coturn
        #if: ${{ false }}
        id: coturn
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v coturn

      - name: üöß Build Images - nmap
        #if: ${{ false }}
        id: nmap
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v nmap

      - name: üöß Build Images - nmap-musl-static
        #if: ${{ false }}
        id: nmap-musl-static
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v nmap-musl-static

      - name: üöß Build Images - s6
        #if: ${{ false }}
        id: s6
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v s6

      - name: üöß Build Images - s6-busybox
        #if: ${{ false }}
        id: s6-busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v s6-busybox

      - name: üöß Build Images - s6-coturn-busybox
        #if: ${{ false }}
        id: s6-coturn-busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v s6-coturn-busybox

      - name: üöß Build Images - tmux
        #if: ${{ false }}
        id: tmux
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v tmux

      - name: üîé Post Build Inspect
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          docker images
          docker ps -a
          df -h
          git status
          git ls-files -o
          ls -ld ~/.kubler
          ls -la ~/.kubler
          ls -ld ~/.kubler/log
          ls -la ~/.kubler/log
          ls -l ~/.kubler/log/build.log || true
          cat ~/.kubler/log/build.log || true
          echo "== Steps"
          printf '%s\n' "$STEPS"
          echo "== Steps | jq"
          printf '%s\n' "$STEPS" | jq .
          echo "== Steps | jq expr"
          printf '%s\n' "$STEPS" | jq 'to_entries | map(select(.value.outputs.kubler == "build").key)'

      - name: üîé Sumarise Build Status
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          echo "# Successful Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")'
          echo "# Failed Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")'

          echo "# Successful Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY
          echo "# Failed Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY

      - run: echo "üçè This job's status is ${{ job.status }}."
