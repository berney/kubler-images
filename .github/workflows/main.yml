name: Kubler Build
on:
  push:
    branches:
      - main
  workflow_dispatch:

# Jobs run in parallel
# Jobs are independent with separate file systems, IP addresses, etc.
jobs:
  kubler:
    runs-on: ubuntu-latest
    steps:
      - run: echo "ðŸŽ‰ The job was automatically triggered by a ${{ github.event_name }} event."
      - run: echo "ðŸ§ This job is now running on a ${{ runner.os }} server hosted by GitHub!"
      - run: echo "ðŸ”Ž The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}."

      - name: Check out repository code
        uses: actions/checkout@v4

      - run: echo "ðŸ’¡ The ${{ github.repository }} repository has been cloned to the runner."
      - run: echo "ðŸ–¥ï¸ The workflow is now ready to test your code on the runner."

      - name: List files in the repository
        run: |
          ls ${{ github.workspace }}

      - name: ðŸ”Ž Inspect Runner
        run: |
          df -h
          free -m
          bash --version
          docker version
          docker info
          pwd
          echo $PATH
          type goss || true

      - name: Install goss
        if: ${{ false }}
        run: |
          # I do build my own goss, but chicken and the egg problem
          curl -sSLO https://github.com/goss-org/goss/releases/latest/download/goss-linux-amd64
          chmod +x goss-linux-amd64
          mv goss-linux-amd64 /usr/local/bin/goss
          type goss
          goss --version
          # Berney: I can't use upstream's `dgoss` as is because it assumes containers are daemon like
          # and doesn't cater to tool like images that do a job and exit - e.g. figlet.
          # So I have modified `dgoss` to suit my needs
          #curl -sSLO https://raw.githubusercontent.com/goss-org/goss/master/extras/dgoss/dgoss
          #chmod +x dgoss
          #mv dgoss /usr/local/bin/
          #type dgoss

      - name: Install goss from berne/goss
        #if: ${{ false }}
        run: |
          # This is a modified goss
          docker pull berne/goss
          docker create --name goss berne/goss
          docker cp goss:/bin/goss /usr/local/bin/
          docker rm goss
          type goss
          goss --version

      - name: Install dgoss
        #if: ${{ false }}
        run: |
          cp dgoss /usr/local/bin/

      - name: Update Docker Engine
        # Disable
        if: ${{ false }}
        run: |
          docker version
          docker info
          # Remove old packages
          for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg || true; done
          sudo apt-get update
          sudo apt-get install ca-certificates curl gnupg
          ls -ld /etc/apt/keyrings || true
          ls -l /etc/apt/keyrings/docker.gpg || true
          sudo install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          sudo chmod a+r /etc/apt/keyrings/docker.gpg
          ls -ld /etc/apt/sources.list.d || true
          ls -l /etc/apt/sources.list.d/ || true
          ls -l /etc/apt/sources.list.d/docker.list || true
          echo \
            "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
            "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
            sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
          cat /etc/apt/sources.list.d/docker.list
          sudo apt-get update
          sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
          systemctl status docker
          sudo systemctl start docker
          docker version
          docker info
          docker run --rm hello-world

      - name: ðŸ‘· Install Kubler
        run: |
          cd .. || exit 1

          ## Install from tarball
          #
          #curl -L https://github.com/edannenberg/kubler/archive/master.tar.gz | tar xz
          #ls -ld kubler-master
          #mv kubler-master kubler
          #ls -l kubler/bin
          #echo "$(pwd)/kubler/bin" >> $GITHUB_PATH

          ## Install from Git
          #
          git clone https://github.com/berney/kubler.git
          cd kubler || exit 1
          # Using my branch to pick-up fixes until PRs are merged
          #git checkout f-berne
          git checkout f-experiment-buildx-bake
          git describe --all --long --dirty
          #git checkout f-berne-bake
          ls -l bin
          echo "$(pwd)/bin" >> $GITHUB_PATH

      - name: ðŸ‘· Kubler Version
        run: |
          export TERM
          kubler --help

      - name: ðŸ‘· Kubler Inspect Terminal
        run: |
          echo "TERM=$TERM"
          echo $PATH
          pwd
          env
          # test term colors
          kubler dep-graph xxx || true
          echo "export TERM"
          export TERM
          kubler dep-graph xxx || true
          echo "export TERM=dumb"
          export TERM
          export TERM=dumb
          kubler dep-graph xxx || true
          echo "export TERM=xterm"
          export TERM=xterm
          kubler dep-graph xxx || true

      # Cache Versions are based off key and path, so differnt path's can use same key
      #
      # Caches are immutable, so need unique key to create a new cache
      # `restore-keys` provides a list to restore a cache when key doesn't match
      # If there's no exact match, the most recent cache that partially matches will be used
      #
      - name: Cache Kubler Downloads
        uses: actions/cache@v4
        with:
          path: ~/.kubler/downloads/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Distfiles
        uses: actions/cache@v4
        with:
          path: ~/.kubler/distfiles/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Cache Kubler Gentoo Packages
        uses: actions/cache@v4
        with:
          path: ~/.kubler/packages/
          key: kubler-${{ github.sha }}
          restore-keys: |
            kubler-

      - name: Set up QEMU
        id: qemu
        uses: docker/setup-qemu-action@v3
        with:
          image: tonistiigi/binfmt:latest
          platforms: all

      - name: ðŸ‹ Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v3
        with:
          # This breaks kubler https://github.com/edannenberg/kubler/issues/215
          # Sets up `docker build` command as an alias to `docker buildx` (default `false`)
          install: true

      - name: ðŸ‹ Inspect builder
        run: |
          echo "Name:      ${{ steps.buildx.outputs.name }}"
          echo "Endpoint:  ${{ steps.buildx.outputs.endpoint }}"
          echo "Status:    ${{ steps.buildx.outputs.status }}"
          echo "Flags:     ${{ steps.buildx.outputs.flags }}"
          echo "Platforms: ${{ steps.buildx.outputs.platforms }}"

      - name: ðŸ‹ Docker Login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
          # this is the default, end of job will logout
          #logout: true

      - name: ðŸ‹ Docker Buildx Inspect
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      # When we use `docker buildx` and `docker bake` we want to be able to access images we built in prior steps
      # For this we need the containerd-snapshotter feature which is in beta
      - name: Docker Daemon Config - Enable Containerd Snapshotter
        run: |
          id -a
          ls -l /etc/docker/daemon.json
          # Old config
          jq . /etc/docker/daemon.json
          # First read old config before we clobber it
          CONFIG=$(jq '.features["containerd-snapshotter"] = true' /etc/docker/daemon.json)
          # Write new config - sudo tee to write to root owned file
          echo "$CONFIG" | sudo tee /etc/docker/daemon.json
          jq . /etc/docker/daemon.json
          systemctl status docker
          sudo systemctl restart docker
          systemctl status docker
          docker run --rm hello-world

      - name: ðŸ‹ Docker Buildx Inspect Again
        run: |
          docker version
          docker info
          docker buildx version
          docker buildx ls
          # `default` is the name of the normie docker builder
          docker buildx inspect default
          # The buildx builder is the default builder due to the `install: true` above
          docker buildx inspect "${{ steps.buildx.outputs.name }}"

      - name: ðŸ‘· Update Kubler (kubler-images, Gentoo Stage3)
        run: |
          export TERM
          ls -la ~/.kubler || true
          ls -la ~/.kubler/namespaces || true
          ls -la ~/.kubler/namespaces/kubler || true
          kubler update
          ls -la ~/.kubler || true
          ls -la ~/.kubler/namespaces || true
          ls -la ~/.kubler/namespaces/kubler || true

      - name: ðŸ‘· Inspect Kubler Images
        run: |
          cd ~/.kubler/namespaces/kubler/ || exit 1
          git remote -v
          git status
          git ls-files -o
          git diff

      - name: ðŸ‘· Get STAGE3_DATE
        run: |
          cd ~/.kubler/namespaces/kubler/ || exit 1
          grep '^STAGE3_DATE=' builder/bob/build.conf
          grep '^STAGE3_DATE=' builder/bob-musl/build.conf
          bob=$(sed -n "s/^STAGE3_DATE='\(202[34][01][0-9]\{3\}T[0-9]\{6\}Z\)'$/\\1/p" builder/bob/build.conf)
          bob_musl=$(sed -n "s/^STAGE3_DATE='\(202[34][01][0-9]\{3\}T[0-9]\{6\}Z\)'$/\\1/p" builder/bob-musl/build.conf)
          if [[ ("$bob" != "$bob_musl") ]]; then
            echo "WARNING: bob and bob-musl have different STAGE3_DATE"
          fi
          echo "BOB_STAGE3_DATE=$bob" >> $GITHUB_ENV
          echo "BOB_MUSL_STAGE3_DATE=$bob_musl" >> $GITHUB_ENV

      - name: ðŸ‘· Fix Kubler Images bob-musl
        # Disable
        if: ${{ false }}
        run: |
          cd ~/.kubler/namespaces/kubler/ || exit 1
          git remote -v
          git remote add berney https://github.com/berney/kubler-images-1.git
          git fetch --all
          git checkout berney/b-fix-util-linux-su-pam
          git diff master..HEAD

      - name: ðŸ‘· Fix Kubler Images busybox
        # Disable
        if: ${{ false }}
        run: |
          cd ~/.kubler/namespaces/kubler/ || exit 1
          git remote -v
          git remote add berney https://github.com/berney/kubler-images-1.git
          git fetch --all
          git checkout berney/f-berne
          git diff master..HEAD

      - name: ðŸ‘· Add Custom Kubler Command(s)
        run: |
          echo "Kubler Commands (OG)"
          pwd
          ls -l
          tree -a -C ../kubler
          ls -l ../kubler
          ls -l ../kubler/cmd
          echo "Custom Kubler Commands"
          ls -l kubler/cmd
          rsync -avi kubler/cmd/* ../kubler/cmd/
          echo "Kubler Commands (now)"
          ls -l ../kubler/cmd

      - name: ðŸ”‘ Get Gentoo Portage GPG Key
        run: |
          # For Portage signatures
          #
          # Fingerprint with spaces `gpg -k --fingerprint --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       Key fingerprint = DCD0 5B71 EAB9 4199 527F  44AC DB6B 8C1F 96D8 BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             Key fingerprint = E1D6 ABB6 3BFC FB4B A02F  DF1C EC59 0EEA C918 9250
          #
          # Fingerprint longkeyid no spaces `gpg -k --with-subkey-fingerprints E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250`:
          #
          # pub   rsa4096/DB6B8C1F96D8BF6D 2011-11-25 [C] [expires: 2023-07-01]
          #       DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D
          #       uid                 [ unknown] Gentoo ebuild repository signing key (Automated Signing Key) <infrastructure@gentoo.org>
          #       uid                 [ unknown] Gentoo Portage Snapshot Signing Key (Automated Signing Key)
          #       sub   rsa4096/EC590EEAC9189250 2011-11-25 [S] [expires: 2023-07-01]
          #             E1D6ABB63BFCFB4BA02FDF1CEC590EEAC9189250
          gpg --keyserver keys.gentoo.org --recv-keys DCD05B71EAB94199527F44ACDB6B8C1F96D8BF6D

          # For Stage3 signatures
          #
          # Fingerprint with spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       Key fingerprint = 13EB BDBE DE7A 1277 5DFD  B1BA BB57 2E0E 2D18 2910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             Key fingerprint = 534E 4209 AB49 EEE1 C19D  9616 2C44 695D B9F6 043D
          #
          # Fingerprint no spaces:
          #
          # pub   rsa4096/BB572E0E2D182910 2009-08-25 [SC] [expires: 2023-07-01]
          #       13EBBDBEDE7A12775DFDB1BABB572E0E2D182910
          #       uid                 [ unknown] Gentoo Linux Release Engineering (Automated Weekly Release Key) <releng@gentoo.org>
          #       sub   rsa2048/2C44695DB9F6043D 2019-02-23 [S] [expires: 2023-07-01]
          #             534E4209AB49EEE1C19D96162C44695DB9F6043D
          #
          gpg --keyserver keys.gentoo.org --recv-keys 13EBBDBEDE7A12775DFDB1BABB572E0E2D182910

          gpg --list-public-keys
          gpg --list-public-keys --with-subkey-fingerprint
          gpg --list-public-keys --with-subkey-fingerprint --fingerprint
          # we just need the key, we don't need to sign/trust it

      - name: ðŸ‘· Kubler Get Latest Portage Date
        run: |
          export TERM
          PORTAGE_DATE=$(kubler portage)
          echo "PORTAGE_DATE=$PORTAGE_DATE" >> $GITHUB_ENV

      - name: ðŸ‘· Kubler Set kubler.conf PORTAGE_DATE
        run: |
          grep PORTAGE_DATE kubler.conf || true
          echo "PORTAGE_DATE=$PORTAGE_DATE" >> kubler.conf

      - name: ðŸ‘· Kubler Set kubler.conf IMAGE_TAG
        run: |
          grep IMAGE_TAG kubler.conf || true
          echo "IMAGE_TAG=$PORTAGE_DATE" >> kubler.conf

      - name: ðŸ‘· Check Kubler Downloads
        run: |
          ls -l ~/.kubler/downloads/portage* || true

      - name: ðŸ”‘ Check GPG
        run: |
          gpg --list-public-keys
          ls -l ~/.kubler/downloads/portage-"${PORTAGE_DATE}".* || true
          if [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ] && [ -e ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz ]; then
            gpg --verify ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz.gpgsig ~/.kubler/downloads/portage-"${PORTAGE_DATE}".tar.xz
          else
            echo "[!] No files to verify"
          fi

      - name: ðŸ‹ðŸ³ Docker Bake Kubler - images before baking
        run: |
          docker images

      - name: ðŸ‹ Use docker buildx instance - before
        if: ${{ false }}
        run: |
          docker buildx ls

      - name: ðŸ‹ Use docker buildx instance
        if: ${{ false }}
        run: |
          # the default docker builder instance can use local images as base images
          # but IIRC it cannot build multi-platform images
          # With the containerd snapshotter it should support multi-platform images
          # It does not support GHA Cache backend
          #
          # Still need this because of `kubler` behaviour with parent images in Dockerfiles.
          docker buildx use default

      - name: ðŸ‹ Use docker buildx instance - after
        if: ${{ false }}
        run: |
          docker buildx ls

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage - set PORTAGE_IMAGE env vars
        run: |
          set -eux
          TMP_PORTAGE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/portage:"${PORTAGE_DATE}"
          PORTAGE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/portage:"${PORTAGE_DATE}"
          echo "TMP_PORTAGE_IMAGE=$TMP_PORTAGE_IMAGE" >> $GITHUB_ENV
          echo "PORTAGE_IMAGE=$PORTAGE_IMAGE" >> $GITHUB_ENV

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage
        if: ${{ false }}
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-portage || exit 1
          # Repeating `--set` adds to the array
          # - https://github.com/docker/buildx/issues/872
          # - Doing it this way results in one image (repository hash) with two tags
          # - doing separate `TAG=xxx docker buildx bake` commands results in two different images (repository hashes), with identical layers, and near identical metadata.
          #docker buildx bake --load --set kubler-portage.tags=kubler-gentoo/portage:latest --set kubler-portage.tags=kubler-gentoo/portage:$PORTAGE_DATE
          # XXX could `--push` here, but can't do both `--load` and `--push` at the same time.
          docker buildx bake --push --set kubler-portage.tags="${TMP_PORTAGE_IMAGE}"

      - name: Check env before exposing GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env before exposing GitHub Runtime - full
        run: |
          env

      # Needed for bake-action to use GHA cache
      # XXX I think this may be a bit dangerous, exposing secrets to everything in the job
      - name: Expose GitHub Runtime
        #if: ${{ false }}
        uses: crazy-max/ghaction-github-runtime@v3

      - name: Check env after exposed GitHub Runtime - grep
        run: |
          env | grep ^ACTIONS

      - name: Check env after exposed GitHub Runtime - full
        run: |
          env

      - name: ðŸ‹ Docker Pull Portage
        id: portage-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$PORTAGE_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage - list targets
        if: ${{ steps.portage-pull.outcome == 'failure' }}
        uses: docker/bake-action/subaction/list-targets@v4
        with:
          workdir: ../kubler/engine/docker/bob-portage

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage
        if: ${{ steps.portage-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: ../kubler/engine/docker/bob-portage
          push: true
          set: |
            kubler-portage.tags=${{ env.TMP_PORTAGE_IMAGE }}
            kubler-portage.cache-from=type=gha,scope=portage
            kubler-portage.cache-to=type=gha,scope=portage

      # We always test, even if we pulled
      # A pushed image should have been tested before being pushed.
      # But re-testing for extra safety and in case tests have changed in between.
      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage - test
        run: |
          set -eux
          docker run --rm "${TMP_PORTAGE_IMAGE}" grep TIMESTAMP /var/db/repos/gentoo/Manifest
          cd portage || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_PORTAGE_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage - skopeo re-tag and push
        if: ${{ false }}
        run: |
          set -eux
          # Simple `docker tag` fails with copying the SLSA attestation.
          #
          #docker tag "${TMP_PORTAGE_IMAGE}" "${PORTAGE_IMAGE}"
          #docker push "${PORTAGE_IMAGE}"
          #
          # To me this is a bug, considering it is docker tools default to add the attestation.
          #
          # Options are either:
          #   1. Use a more powerful tool:
          #       - skopeo - https://github.com/containers/skopeo
          #       - oras - https://github.com/oras-project/oras
          #       - crane - https://github.com/google/go-containerregistry (GGCR)
          #       - regctl - https://github.com/regclient/regclient
          #       - maybe img from containerd
          #   2. Rebuild againt with different name/tag, which should use the cache
          #
          # BTW skopeo inspect is povo mixing manifest and arch
          # `sudo apt-get update` is in previous step earlier - but that step is disabled
          sudo apt-get update
          sudo apt-get install -y skopeo
          # Without `--all` it won't copy the attestation, and will flatten the manifest
          skopeo copy --all docker://"${TMP_PORTAGE_IMAGE}" docker://"${PORTAGE_IMAGE}"
          # Fails with this error:
          # time="2024-01-28T12:18:57Z" level=fatal msg="creating an updated image manifest: preparing updated manifest, layer \"sha256:d70a7eb0a9bff363aec36a597932c7a76ef49dd763f61ba4d690ff979163323b\": unsupported MIME type for compression: application/vnd.in-toto+json"

      - name: Install oras
        run: |
          set -eux
          # Simple `docker tag` gets an error on the attestation layer
          # `skopeo copy` works but strips the attestation
          # `skopeo copy --all` attempts to copy the attestation and gets an error because it doesn't support the MIME type
          VERSION="1.1.0"
          KEYID="BE6F A8DD A48D 4C23 0091  A0A9 276D 8A72 4CE1 C704"
          FILE="oras_${VERSION}_linux_amd64.tar.gz"
          SIG_FILE="oras_${VERSION}_linux_amd64.tar.gz.asc"
          curl -sSf https://github.com/qweeah.gpg | gpg --import
          curl -LO "https://github.com/oras-project/oras/releases/download/v${VERSION}/${FILE}"
          curl -LO "https://github.com/oras-project/oras/releases/download/v${VERSION}/${SIG_FILE}"
          verify_output=$(gpg --verify "$SIG_FILE" "$FILE" 2>&1)
          if echo "$verify_output" | grep -q "^gpg: Good signature from "; then
            echo "Signature is good"
          else
            echo "Signature verification failed"
            exit 1
          fi
          if echo "$verify_output" | grep -q "^Primary key fingerprint: ${KEYID}$";  then
            echo "Signed by expected key"
          else
            echo "Signature not made by expected key"
            exit 1
          fi
          tar -zxf oras_${VERSION}_*.tar.gz -C /usr/local/bin/ oras
          rm "${FILE}" "${SIG_FILE}"
          type oras
          oras version

      - name: ðŸ‹ðŸ³ Docker Bake Kubler portage - oras cp
        if: ${{ steps.portage-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_PORTAGE_IMAGE" "$PORTAGE_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler - images portage
        run: |
          docker images

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - set ENV vars
        run: |
          set -eux
          # XXX Not the tags I'm setting here, and the BASE_TAG I use later probably have a disconnect
          TMP_STAGE3_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/stage3-amd64-hardened-nomultilib-openrc:"${PORTAGE_DATE}"
          echo "TMP_STAGE3_IMAGE=$TMP_STAGE3_IMAGE" >> $GITHUB_ENV
          TMP_STAGE3_MUSL_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/stage3-amd64-musl-hardened:"${PORTAGE_DATE}"
          echo "TMP_STAGE3_MUSL_IMAGE=$TMP_STAGE3_MUSL_IMAGE" >> $GITHUB_ENV
          STAGE3_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/stage3-amd64-hardened-nomultilib-openrc:"${PORTAGE_DATE}"
          echo "STAGE3_IMAGE=$STAGE3_IMAGE" >> $GITHUB_ENV
          STAGE3_MUSL_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/stage3-amd64-musl-hardened:"${PORTAGE_DATE}"
          echo "STAGE3_MUSL_IMAGE=$STAGE3_MUSL_IMAGE" >> $GITHUB_ENV

      - name: ðŸ‹ Docker Pull stage3
        id: stage3-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_IMAGE"

      - name: ðŸ‹ Docker Pull stage3 musl
        id: stage3-musl-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$STAGE3_MUSL_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3
        if: ${{ false }}
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-stage3 || exit 1
          # XXX There is a disconnect here between base image this would use and the tag we give it
          # Base image would be gentoo/stage3:musl-hardened (e.g. latest musl hardened image)
          # Could use `BASE_TAG=musl-hardened-20240102` (YYYYMMDD) to pin.
          # The stage3 date we tag it with might not be correct
          #
          # XXX These will default to using PORTAGE=gentoo/portage
          # But Kubler portage applies some patches
          # This is working because `kubler build` is mounting portage into container.
          # We could set `PORTAGE` to the kubler-portage image, so that the patches are embedded.
          BASE_TAG=hardened-nomultilib \
            PORTAGE=$PORTAGE_IMAGE \
            docker buildx bake \
              --push \
              --set gentoo-stage3.tags="${TMP_STAGE3_IMAGE}"
          BASE_TAG=musl-hardened \
            PORTAGE=$PORTAGE_IMAGE \
            docker buildx bake \
              --push \
              --set gentoo-stage3.tags="${TMP_STAGE3_MUSL_IMAGE}"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - list targets
        if: ${{ steps.stage3-pull.outcome == 'failure' }}
        uses: docker/bake-action/subaction/list-targets@v4
        with:
          workdir: ../kubler/engine/docker/bob-stage3

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - hardened-nomultilib
        if: ${{ steps.stage3-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: ../kubler/engine/docker/bob-stage3
          push: true
          # In the `docker-bake.hcl`
          # There is a variable `BASE_TAG` and `BASE_IMAGE`. `BASE_IMAGE` interpolates `BASE_TAG`i
          # These can be set with env vars when using `docker buildx bake` command
          # For the gentoo-stage3 target there's no `BASE_TAG` arg, only `BASE_IMAGE` which defaults to `BASE_IMAGE` var
          # When using bake-action we need to set the `BASE_IMAGE` arg, setting `BASE_TAG` arg won't do what we want.
          set: |
            gentoo-stage3.args.BASE_IMAGE=gentoo/stage3:hardened-nomultilib
            gentoo-stage3.args.PORTAGE=${{ env.PORTAGE_IMAGE }}
            gentoo-stage3.tags=${{ env.TMP_STAGE3_IMAGE }}
            gentoo-stage3.cache-from=type=gha,scope=portage
            gentoo-stage3.cache-from=type=gha,scope=stage3-hardened-nomultilib
            gentoo-stage3.cache-to=type=gha,scope=stage3-hardened-nomultilib

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - test
        run: |
          set -eux
          docker run --rm "${TMP_STAGE3_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_STAGE3_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_STAGE3_IMAGE}" eselect profile show
          cd stage3/amd64-hardened-nomultilib-openrc || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_STAGE3_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - oras cp
        if: ${{ steps.stage3-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_STAGE3_IMAGE" "$STAGE3_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 - musl-hardened
        if: ${{ steps.stage3-musl-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: ../kubler/engine/docker/bob-stage3
          push: true
          # In the `docker-bake.hcl`
          # There is a variable `BASE_TAG` and `BASE_IMAGE`. `BASE_IMAGE` interpolates `BASE_TAG`i
          # These can be set with env vars when using `docker buildx bake` command
          # For the gentoo-stage3 target there's no `BASE_TAG` arg, only `BASE_IMAGE` which defaults to `BASE_IMAGE` var
          # When using bake-action we need to set the `BASE_IMAGE` arg, setting `BASE_TAG` arg won't do what we want.
          set: |
            gentoo-stage3.args.BASE_IMAGE=gentoo/stage3:musl-hardened
            gentoo-stage3.args.PORTAGE=${{ env.PORTAGE_IMAGE }}
            gentoo-stage3.tags=${{ env.TMP_STAGE3_MUSL_IMAGE }}
            gentoo-stage3.cache-from=type=gha,scope=portage
            gentoo-stage3.cache-from=type=gha,scope=stage3-musl-hardened
            gentoo-stage3.cache-to=type=gha,scope=stage3-musl-hardened

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 musl - test
        run: |
          set -eux
          docker run --rm "${TMP_STAGE3_MUSL_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_STAGE3_MUSL_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_STAGE3_MUSL_IMAGE}" eselect profile show
          cd stage3/amd64-musl-hardened || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_STAGE3_MUSL_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler stage3 musl - oras cp
        if: ${{ steps.stage3-musl-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_STAGE3_MUSL_IMAGE" "$STAGE3_MUSL_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler - images stage3
        run: |
          docker images

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core - set ENV vars
        run: |
          set -eux
          TMP_BOB_CORE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/bob-core:"${PORTAGE_DATE}"
          echo "TMP_BOB_CORE_IMAGE=$TMP_BOB_CORE_IMAGE" >> $GITHUB_ENV
          TMP_BOB_MUSL_CORE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/bob-musl-core:"${PORTAGE_DATE}"
          echo "TMP_BOB_MUSL_CORE_IMAGE=$TMP_BOB_MUSL_CORE_IMAGE" >> $GITHUB_ENV
          BOB_CORE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/bob-core:"${PORTAGE_DATE}"
          echo "BOB_CORE_IMAGE=$BOB_CORE_IMAGE" >> $GITHUB_ENV
          BOB_MUSL_CORE_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/bob-musl-core:"${PORTAGE_DATE}"
          echo "BOB_MUSL_CORE_IMAGE=$BOB_MUSL_CORE_IMAGE" >> $GITHUB_ENV

      - name: ðŸ‹ Docker Pull bob-core
        id: bob-core-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_CORE_IMAGE"

      - name: ðŸ‹ Docker Pull bob-musl-core
        id: bob-musl-core-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_CORE_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core
        if: ${{ false }}
        run: |
          set -eux
          cd ../kubler/engine/docker/bob-core || exit 1
          BASE_IMAGE="${STAGE3_IMAGE}" \
            docker buildx bake \
              --push  \
              --set core.tags="${TMP_BOB_CORE_IMAGE}"
          DEF_CHOST=x86_64-gentoo-linux-musl \
            BASE_IMAGE="${STAGE3_MUSL_IMAGE}" \
            docker buildx bake \
              --push \
              --set core.tags="${TMP_BOB_MUSL_CORE_IMAGE}"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core - list targets
        if: ${{ steps.bob-core-pull.outcome == 'failure' }}
        uses: docker/bake-action/subaction/list-targets@v4
        with:
          workdir: ../kubler/engine/docker/bob-core

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core
        if: ${{ steps.bob-core-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: ../kubler/engine/docker/bob-core
          push: true
          set: |
            core.args.BASE_IMAGE=${{ env.STAGE3_IMAGE }}
            core.tags=${{ env.TMP_BOB_CORE_IMAGE }}
            core.cache-from=type=gha,scope=portage
            core.cache-from=type=gha,scope=stage3-hardened-nomultilib
            core.cache-from=type=gha,scope=bob-core
            core.cache-to=type=gha,scope=bob-core

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core - test
        run: |
          set -eux
          docker run --rm "${TMP_BOB_CORE_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_BOB_CORE_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_BOB_CORE_IMAGE}" eselect profile show
          docker run --rm "${TMP_BOB_CORE_IMAGE}" ls -l /var/cache
          cd core/bob-core || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_BOB_CORE_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-core - oras cp
        if: ${{ steps.bob-core-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_BOB_CORE_IMAGE" "$BOB_CORE_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-musl-core
        if: ${{ steps.bob-musl-core-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: ../kubler/engine/docker/bob-core
          push: true
          set: |
            core.args.DEF_CHOST=x86_64-gentoo-linux-musl
            core.args.BASE_IMAGE=${{ env.STAGE3_MUSL_IMAGE }}
            core.tags=${{ env.TMP_BOB_MUSL_CORE_IMAGE }}
            core.cache-from=type=gha,scope=portage
            core.cache-from=type=gha,scope=stage3-musl-hardened
            core.cache-from=type=gha,scope=bob-musl-core
            core.cache-to=type=gha,scope=bob-musl-core


      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-musl-core - test
        run: |
          set -eux
          docker run --rm "${TMP_BOB_MUSL_CORE_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_BOB_MUSL_CORE_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_BOB_MUSL_CORE_IMAGE}" eselect profile show
          docker run --rm "${TMP_BOB_MUSL_CORE_IMAGE}" ls -l /var/cache
          cd core/bob-musl-core || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_BOB_MUSL_CORE_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler bob-musl-core - oras cp
        if: ${{ steps.bob-musl-core-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_BOB_MUSL_CORE_IMAGE" "$BOB_MUSL_CORE_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Bake Kubler - images bob-core
        run: |
          docker images

      - name: ðŸ‹ Docker Build kubler builder - bob - set ENV vars
        run: |
          set -eux
          TMP_BOB_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/bob:"${PORTAGE_DATE}"
          echo "TMP_BOB_IMAGE=$TMP_BOB_IMAGE" >> $GITHUB_ENV
          BOB_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/bob:"${PORTAGE_DATE}"
          echo "BOB_IMAGE=$BOB_IMAGE" >> $GITHUB_ENV
          TMP_BOB_MUSL_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/tmp/bob-musl:"${PORTAGE_DATE}"
          echo "TMP_BOB_MUSL_IMAGE=$TMP_BOB_MUSL_IMAGE" >> $GITHUB_ENV
          BOB_MUSL_IMAGE=ghcr.io/"${GITHUB_REPOSITORY}"/bob-musl:"${PORTAGE_DATE}"
          echo "BOB_MUSL_IMAGE=$BOB_MUSL_IMAGE" >> $GITHUB_ENV

      - name: ðŸ‹ Docker Pull bob
        id: bob-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_IMAGE"

      - name: ðŸ‹ Docker Pull bob-musl
        id: bob-musl-pull
        continue-on-error: true
        run: |
          set -eu
          docker pull "$BOB_MUSL_IMAGE"

      - name: ðŸ‹ Docker Build kubler builder - bob
        if: ${{ false }}
        run: |
          set -eux
          # Build kubler builder outside of kubler with native Docker tools
          # So that we can use the buildkit docker-container driver that supports GHA caching
          cd builder/bob || exit 1
          # This dockerfile does similar to what kubler does to build a builder
          # Difference is kubler uses docker run and mounts volumes, and then commits the container as an image
          # We are building direct, so can't use volumes.
          # I am hoping caching makes up for that.
          docker buildx build \
            --push \
            -f Dockerfile.berney \
            --build-arg PARENT="${BOB_CORE_IMAGE}" \
            -t "${TMP_BOB_IMAGE}" \
            .

      - name: ðŸ‹ Docker Build kubler builder - bob - list targets
        if: ${{ steps.bob-pull.outcome == 'failure' }}
        uses: docker/bake-action/subaction/list-targets@v4
        with:
          workdir: builder/bob

      - name: ðŸ‹ðŸ³ Docker Bake Kubler builder - bob
        if: ${{ steps.bob-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: builder/bob
          push: true
          set: |
            bob.args.BASE_IMAGE=${{ env.BOB_CORE_IMAGE }}
            bob.tags=${{ env.TMP_BOB_IMAGE }}
            bob.cache-from=type=gha,scope=portage
            bob.cache-from=type=gha,scope=stage3-hardened-nomultilib
            bob.cache-from=type=gha,scope=bob-core
            bob.cache-from=type=gha,scope=bob
            bob.cache-to=type=gha,scope=bob

      - name: ðŸ‹ Docker Build kubler builder - bob - test
        run: |
          set -eux
          docker run --rm "${TMP_BOB_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_BOB_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_BOB_IMAGE}" eselect profile show
          docker run --rm "${TMP_BOB_IMAGE}" ls -l /var/cache
          docker run --rm "${TMP_BOB_IMAGE}" ls -l /var/cache/eix
          docker run --rm "${TMP_BOB_IMAGE}" eix --selected -c
          cd builder/bob || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_BOB_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler builder - bob - oras cp
        if: ${{ steps.bob-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_BOB_IMAGE" "$BOB_IMAGE"

      - name: ðŸ‹ Docker Build kubler builder - bob-musl
        if: ${{ false }}
        run: |
          set -eux
          # Build kubler builder outside of kubler with native Docker tools
          # So that we can use the buildkit docker-container driver that supports GHA caching
          cd builder/bob-musl || exit 1
          # This dockerfile does similar to what kubler does to build a builder
          # Difference is kubler uses docker run and mounts volumes, and then commits the container as an image
          # We are building direct, so can't use volumes.
          # I am hoping caching makes up for that.
          docker buildx build \
            --push \
            -f Dockerfile.berney \
            --build-arg PARENT="${BOB_MUSL_CORE_IMAGE}" \
            -t "${TMP_BOB_MUSL_IMAGE}" \
            .

      - name: ðŸ‹ðŸ³ Docker Bake Kubler builder - bob-musl
        if: ${{ steps.bob-musl-pull.outcome == 'failure' }}
        uses: docker/bake-action@v4
        with:
          workdir: builder/bob-musl
          push: true
          set: |
            bob-musl.args.BASE_IMAGE=${{ env.BOB_MUSL_CORE_IMAGE }}
            bob-musl.tags=${{ env.TMP_BOB_MUSL_IMAGE }}
            bob-musl.cache-from=type=gha,scope=portage
            bob-musl.cache-from=type=gha,scope=stage3-musl-hardened
            bob-musl.cache-from=type=gha,scope=bob-musl-core
            bob-musl.cache-from=type=gha,scope=bob-musl
            bob-musl.cache-to=type=gha,scope=bob-musl


      - name: ðŸ‹ Docker Build kubler builder - bob-musl - test
        run: |
          set -eux
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" cat /etc/gentoo-release
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" sh -c "grep -E 'Latest|stage3' /latest-stage3*.txt"
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" eselect profile show
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" ls -l /var/cache/eix
          docker run --rm "${TMP_BOB_MUSL_IMAGE}" eix --selected -c
          cd builder/bob-musl || exit 1
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss "${TMP_BOB_MUSL_IMAGE}" validate --format documentation --color

      - name: ðŸ‹ðŸ³ Docker Bake Kubler builder - bob-musl - oras cp
        if: ${{ steps.bob-musl-pull.outcome == 'failure' }}
        run: |
          set -eux
          oras cp -v "$TMP_BOB_MUSL_IMAGE" "$BOB_MUSL_IMAGE"

      - name: ðŸ‹ðŸ³ Docker Images
        run: |
          docker images

      - name: ðŸ‹ðŸ³ Docker Tag Images for Kubler Compatability
        run: |
          # We need a local copy to tag
          # We build tmp image and use oras to copy it, so we don't have it locally
          docker pull "${PORTAGE_IMAGE}"
          docker pull "${BOB_CORE_IMAGE}"
          docker pull "${BOB_MUSL_CORE_IMAGE}"
          docker pull "${BOB_IMAGE}"
          docker pull "${BOB_MUSL_IMAGE}"
          docker tag "${PORTAGE_IMAGE}" kubler-gentoo/portage:latest
          docker tag "${BOB_CORE_IMAGE}" kubler/bob-core:"${PORTAGE_DATE}"
          docker tag "${BOB_MUSL_CORE_IMAGE}" kubler/bob-musl-core:"${PORTAGE_DATE}"
          docker tag "${BOB_IMAGE}" kubler/bob:"${PORTAGE_DATE}"
          docker tag "${BOB_MUSL_IMAGE}" kubler/bob-musl:"${PORTAGE_DATE}"
          docker images

      - name: ðŸ‹ðŸ³ Docker Images
        run: |
          docker images

      - name: ðŸ‘· Kubler Set kubler.conf DEFAULT_MUSL_BUILDER
        #if: ${{ false }}
        run: |
          grep DEFAULT_MUSL_BUILDER kubler.conf || true
          echo "DEFAULT_MUSL_BUILDER=$BOB_MUSL_IMAGE" >> kubler.conf

      - name: ðŸ‘· Portage Container - eix-update - Populate /var/cache/eix
        run: |
          set -u
          # Kubler building builders would use volume from portage container
          # and run `eix-update` which would populate /var/cache/eix.
          # But I'm building the builders directly with docker
          # Docker build doesn't support mounting volumes during builds
          # When using `kubler build` to build images with our builder,
          # the builds fail with this error:
          #
          #   cannot open database file /var/cache/eix/portage.eix for reading
          #
          # So this step will fix this to make kubler build for images happy.
          # kubler does `docker run`, but I think `docker create` is more idiomatic
          docker create --name kubler-gentoo-portage "${PORTAGE_IMAGE}" true
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" eix-update
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache
          docker run --rm --volumes-from kubler-gentoo-portage "${BOB_MUSL_IMAGE}" ls -l /var/cache/eix

      - name: ðŸš§ Build Images - berney/busybox
        #if: ${{ false }}
        id: busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          # This is my copy with small changes
          kubler build -v busybox

      - name: Test Image - berney/busybox
        id: test-busybox
        continue-on-error: true
        run: |
          cd images/busybox || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/busybox validate --format documentation --color

      - name: ðŸš§ Build Images - kubler/busybox
        #if: ${{ false }}
        id: kubler-busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v kubler/busybox

      - name: Test Image - kubler/busybox
        id: test-kubler-busybox
        continue-on-error: true
        run: |
          cd images/busybox || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler/busybox validate --format documentation --color

      #- name: ðŸ›‘ STOP HERE
      #  run: |
      #    false

      - name: ðŸš§ Build Images - kubler/glibc
        #if: ${{ false }}
        id: glibc
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v kubler/glibc

      - name: Test Image - kubler/glibc
        id: test-kubler-glibc
        continue-on-error: true
        run: |
          cd images/glibc || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler/glibc validate --format documentation --color

      - name: ðŸš§ Build Images - figlet
        #if: ${{ false }}
        id: figlet
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet

      - name: Test Image - figlet
        id: test-figlet
        continue-on-error: true
        run: |
          cd images/figlet || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/figlet validate --format documentation --color

      - name: ðŸš§ Build Images - figlet-user
        #if: ${{ false }}
        id: figlet-user
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-user

      - name: Test Image - figlet-user
        id: test-figlet-user
        continue-on-error: true
        run: |
          cd images/figlet-user || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/figlet-user validate --format documentation --color

      - name: ðŸš§ Build Images - figlet-musl
        #if: ${{ false }}
        id: figlet-musl
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-musl

      - name: Test Image - figlet-musl
        id: test-figlet-musl
        continue-on-error: true
        run: |
          cd images/figlet-musl || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/figlet-musl validate --format documentation --color

      - name: ðŸš§ Build Images - figlet-musl-static
        #if: ${{ false }}
        id: figlet-musl-static
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v figlet-musl-static

      - name: Test Image - figlet-musl-static
        id: test-figlet-musl-static
        continue-on-error: true
        run: |
          cd images/figlet-musl-static || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/figlet-musl-static validate --format documentation --color

      - name: ðŸš§ Build Images - goss
        #if: ${{ false }}
        id: goss
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v goss

      - name: Test Image - goss
        id: test-goss
        continue-on-error: true
        run: |
          cd images/goss || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          # Because the image is already using `/goss` (and it's a file), we need `dgoss` to use a different directory
          # I've added teh `CONTAINER_GOSS_PATH` env var to allow overriding the default
          CONTAINER_MODE=entrypoint CONTAINER_GOSS_PATH=/goss2 dgoss run -w /goss2 --entrypoint /goss2/goss kubler-images/goss validate --format documentation --color

      - name: ðŸš§ Build Images - fd
        #if: ${{ false }}
        id: fd
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v fd

      - name: ðŸš§ Build Images - s6
        #if: ${{ false }}
        id: s6
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build -v s6

      - name: Test Image - s6
        id: test-s6
        continue-on-error: true
        run: |
          cd images/s6 || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          CONTAINER_MODE=entrypoint dgoss run -w /goss --entrypoint /goss/goss kubler-images/s6 validate --format documentation --color

      - name: ðŸš§ Build Images - s6 - push
        #if: ${{ false }}
        id: s6-push
        continue-on-error: true
        run: |
          # The buildx docker-container driver can't pull from the local registry
          # So we need to push the image
          docker tag kubler-images/s6 ghcr.io/berney/kubler-images/s6
          docker push ghcr.io/berney/kubler-images/s6

      - name: ðŸš§ Build Images - s6-busybox
        #if: ${{ false }}
        id: s6-busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=s6-busybox"\
          " --cache-to=type=gha,scope=s6-busybox" \
          kubler build -v s6-busybox

      - name: Test Image - s6-busybox
        id: test-s6-busybox
        continue-on-error: true
        run: |
          cd images/s6-busybox || false
          echo "kubler=test" >> $GITHUB_OUTPUT
          # ash needs a terminal to start
          dgoss run -t kubler-images/s6-busybox

      - name: ðŸš§ Build Images - s6-busybox - push
        #if: ${{ false }}
        id: s6-busybox-push
        continue-on-error: true
        run: |
          # The buildx docker-container driver can't pull from the local registry
          # So we need to push the image
          docker tag kubler-images/s6-busybox ghcr.io/berney/kubler-images/s6-busybox
          docker push ghcr.io/berney/kubler-images/s6-busybox

      # This is based off s6
      - name: ðŸš§ Build Images - coturn
        #if: ${{ false }}
        id: coturn
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          # To continue the string, no spaces between end quote,slash, EOL, or start of line
          # e.g. as a stream its `"alpha"\\\n" omega"\\\n"
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=coturn"\
          " --cache-to=type=gha,scope=coturn" \
            kubler build -v coturn

      - name: ðŸš§ Build Images - s6-coturn-busybox
        #if: ${{ false }}
        id: s6-coturn-busybox
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=coturn"\
          " --cache-from=type=gha,scope=s6-busybox"\
          " --cache-from=type=gha,scope=s6-coturn-busybox"\
          " --cache-to=type=gha,scope=s6-coturn-busybox" \
            kubler build -v s6-coturn-busybox

      - name: ðŸš§ Build Images - nmap
        #if: ${{ false }}
        id: nmap
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=nmap"\
          " --cache-to=type=gha,scope=nmap" \
            kubler build -v nmap

      - name: ðŸš§ Build Images - nmap-musl-static
        #if: ${{ false }}
        id: nmap-musl-static
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=nmap-musl-static"\
          " --cache-to=type=gha,scope=nmap-musl-static" \
            kubler build -v nmap-musl-static

      - name: ðŸš§ Build Images - tmux
        #if: ${{ false }}
        id: tmux
        continue-on-error: true
        run: |
          echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          DOCKER_BUILD_OPTS="--load"\
          " --cache-from=type=gha,scope=tmux"\
          " --cache-to=type=gha,scope=tmux" \
            kubler build -v tmux

      - name: ðŸš§ Build Images - Everything excluding Problematic
        #if: ${{ false }}
        id: everything-exc-problematic
        continue-on-error: true
        run: |
          #echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build kubler-images -e kubler-images/nmap-musl-static

      - name: ðŸš§ Build Images - EVERYTHING
        if: ${{ false }}
        id: everything
        continue-on-error: true
        run: |
          #echo "kubler=build" >> $GITHUB_OUTPUT
          export TERM
          kubler build kubler-images

      - name: ðŸ”Ž Post Build Inspect
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          docker images
          docker ps -a
          df -h
          git status
          git ls-files -o
          ls -ld ~/.kubler
          ls -la ~/.kubler
          ls -ld ~/.kubler/log
          ls -la ~/.kubler/log
          ls -l ~/.kubler/log/build.log || true
          cat ~/.kubler/log/build.log || true
          echo "== Steps"
          printf '%s\n' "$STEPS"
          echo "== Steps | jq"
          printf '%s\n' "$STEPS" | jq .
          echo "== Steps | jq expr"
          printf '%s\n' "$STEPS" | jq 'to_entries | map(select(.value.outputs.kubler == "build").key)'

      - name: ðŸ”Ž Sumarise Build Status
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          echo "# Successful Build Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")'
          echo "# Failed Build Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")'

          echo "# Successful Build Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome == "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY
          echo "# Failed Build Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "build" and .outcome != "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY

      - name: ðŸ”Ž Sumarise Test Status
        env:
          # This will contain double-quotes, which `echo` would eat, breaking `jq`
          # https://stackoverflow.com/a/72955840
          STEPS: ${{ toJSON(steps) }}
        run: |
          echo "# Successful Test Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome == "success").key | sub("^"; "* ")) | join("\n")'
          echo "# Failed Test Steps"
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome != "success").key | sub("^"; "* ")) | join("\n")'

          echo "# Successful Test Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome == "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY
          echo "# Failed Test Steps" >> $GITHUB_STEP_SUMMARY
          printf '%s\n' "$STEPS" | jq -r 'to_entries | map(select(.value | .outputs.kubler == "test" and .outcome != "success").key | sub("^"; "* ")) | join("\n")' >> $GITHUB_STEP_SUMMARY

      - run: echo "ðŸ This job's status is ${{ job.status }}."
